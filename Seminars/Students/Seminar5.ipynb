{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eb8sjVZD9rV9",
        "IciTDSNW0HnZ"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1: Deep Dive into Transformers for Text.\n",
        "\n",
        "**Goal:** Understand the inner workings of the Transformer architecture for Natural Language Processing (NLP) tasks. We will explore tokenization, embeddings, positional encoding, self-attention (encoder & decoder), cross-attention, and how these components fit together.\n",
        "\n",
        "**Approach:** We will use a combination of theoretical explanations, visualizations, and hands-on code examples. We will leverage the Hugging Face `transformers` library to inspect a pre-trained model and also implement key components from scratch using PyTorch to solidify understanding. We will **not** be training a model, but rather dissecting an existing one.\n",
        "\n",
        "> For a visual overview of the Transformer architecture (**HIGHLY RECOMMENDED**), please see:\n",
        "> * https://jalammar.github.io/illustrated-transformer/\n",
        "> * https://poloclub.github.io/transformer-explainer/  \n",
        "> ![Transformer Architecture](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)\n",
        "\n",
        "\n",
        "**Let's get started!**"
      ],
      "metadata": {
        "id": "_1Kzj2uy9aaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "eb8sjVZD9rV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch matplotlib seaborn numpy pandas plotly\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import logging as hf_logging\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "\n",
        "# Set seed for reproducibility (optional)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Suppress verbose Hugging Face warnings\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Check for GPU availability (optional, but good practice)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VELsOl0y7mz",
        "outputId": "e699e63b-e0bb-49af-fc49-309a0951f453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize tokenization\n",
        "def visualize_tokenization(text, tokenizer):\n",
        "    # Get regular tokens and their IDs (without special tokens)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    df = pd.DataFrame({\n",
        "        'Token': tokens,\n",
        "        'ID': token_ids\n",
        "    })\n",
        "\n",
        "    # Get input IDs including special tokens (e.g., [CLS] and [SEP] for BERT)\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Create a figure for visualization\n",
        "    num_rows = len(df) + 1  # +1 for the header row\n",
        "    fig, ax = plt.subplots(figsize=(8, num_rows * 0.5 + 2))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Generate a table with token data\n",
        "    table = ax.table(cellText=df.values,\n",
        "                     colLabels=df.columns,\n",
        "                     cellLoc='center',\n",
        "                     loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 1.5)\n",
        "\n",
        "    # Add a title on top\n",
        "    plt.title(f\"Tokenization Visualization for:\\n{text}\", fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Display information about special tokens at the bottom of the figure\n",
        "    plt.figtext(0.5, 0.01,\n",
        "                f\"Tokens with special tokens: {all_tokens}\\nInput IDs with special tokens: {input_ids}\",\n",
        "                wrap=True, horizontalalignment='center', fontsize=10)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# --- Interactive Plot with Plotly (Optional, but nice!) ---\n",
        "def plot_attention_interactive(attention_layer_tensor, tokens, layer_idx):\n",
        "    \"\"\"Creates an interactive heatmap using Plotly, allowing head selection.\"\"\"\n",
        "    num_heads = attention_layer_tensor.shape[1]\n",
        "    seq_len = attention_layer_tensor.shape[2]\n",
        "\n",
        "    fig = make_subplots(rows=1, cols=1)\n",
        "\n",
        "    # Add traces for each head, initially visible=False except the first\n",
        "    for h in range(num_heads):\n",
        "        fig.add_trace(\n",
        "            go.Heatmap(\n",
        "                z=attention_layer_tensor[0, h, :, :].numpy(), # Select batch 0, head h\n",
        "                x=tokens,\n",
        "                y=tokens,\n",
        "                colorscale='Viridis',\n",
        "                name=f'Head {h}',\n",
        "                visible=(h == 0), # Only first head visible initially\n",
        "                showscale=False # Hide individual color bars\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "    # Create dropdown menu to select head\n",
        "    buttons = []\n",
        "    for h in range(num_heads):\n",
        "        visibility = [False] * num_heads\n",
        "        visibility[h] = True\n",
        "        buttons.append(dict(\n",
        "            label=f'Head {h}',\n",
        "            method='update',\n",
        "            args=[{'visible': visibility},\n",
        "                  {'title': f'Self-Attention Weights - Layer {layer_idx}, Head {h}'}]\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        updatemenus=[dict(\n",
        "            active=0,\n",
        "            buttons=buttons,\n",
        "            direction=\"down\",\n",
        "            pad={\"r\": 10, \"t\": 10},\n",
        "            showactive=True,\n",
        "            x=0.1,\n",
        "            xanchor=\"left\",\n",
        "            y=1.15,\n",
        "            yanchor=\"top\"\n",
        "        )],\n",
        "        title=f'Self-Attention Weights - Layer {layer_idx}, Head 0',\n",
        "        xaxis_title=\"Key (Attended To)\",\n",
        "        yaxis_title=\"Query (Attending From)\",\n",
        "        yaxis_autorange='reversed', # Put [CLS] at the top-left\n",
        "        height=700,\n",
        "        width=700\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "228_bRZ1IN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_heatmap_improved(attention_matrix, x_labels, y_labels, title, figsize=(12, 10), cmap=\"viridis\", fmt=\".2f\"):\n",
        "    \"\"\"\n",
        "    Plots a clearer heatmap for attention weights using Matplotlib/Seaborn.\n",
        "\n",
        "    Args:\n",
        "        attention_matrix (np.array): 2D numpy array of attention weights.\n",
        "        x_labels (list): List of labels for the X-axis (Keys/Values).\n",
        "        y_labels (list): List of labels for the Y-axis (Queries).\n",
        "        title (str): Title for the plot.\n",
        "        figsize (tuple): Figure size.\n",
        "        cmap (str): Colormap name.\n",
        "        fmt (str): String format for annotations (e.g., '.2f' for 2 decimal places). Set to None to disable annotations.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(\n",
        "        attention_matrix,\n",
        "        xticklabels=x_labels,\n",
        "        yticklabels=y_labels,\n",
        "        cmap=cmap,\n",
        "        linewidths=.5,\n",
        "        linecolor='lightgray', # Add lines between cells\n",
        "        cbar=True,           # Show color bar\n",
        "        annot= (attention_matrix.shape[0] < 20 and attention_matrix.shape[1] < 20 and fmt is not None), # Show annotations only for smaller matrices\n",
        "        fmt=fmt,\n",
        "        annot_kws={\"size\": 8} # Adjust annotation font size if needed\n",
        "    )\n",
        "    plt.xlabel(\"Encoder Tokens (Keys / Values from Source)\", fontsize=12)\n",
        "    plt.ylabel(\"Decoder Tokens (Queries from Target)\", fontsize=12)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(rotation=0, fontsize=10)\n",
        "    plt.tight_layout(pad=1.5) # Adjust layout to prevent overlap\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cross_attention_interactive(attention_layer_tensor, encoder_tokens, decoder_tokens, layer_idx):\n",
        "    \"\"\"\n",
        "    Creates an interactive heatmap specifically for CROSS-attention using Plotly.\n",
        "\n",
        "    Args:\n",
        "        attention_layer_tensor (torch.Tensor): Attention weights tensor for a layer.\n",
        "                                              Shape: (batch_size, num_heads, target_seq_len, source_seq_len)\n",
        "        encoder_tokens (list): List of tokens for the source sequence (X-axis).\n",
        "        decoder_tokens (list): List of tokens for the target sequence (Y-axis).\n",
        "        layer_idx (int): The index of the decoder layer being visualized.\n",
        "    \"\"\"\n",
        "    num_heads = attention_layer_tensor.shape[1]\n",
        "    # Ensure tensor is on CPU and NumPy for plotting\n",
        "    attention_layer_np = attention_layer_tensor.cpu().numpy()\n",
        "\n",
        "    fig = make_subplots(rows=1, cols=1)\n",
        "\n",
        "    # Add traces for each head\n",
        "    for h in range(num_heads):\n",
        "        fig.add_trace(\n",
        "            go.Heatmap(\n",
        "                z=attention_layer_np[0, h, :, :], # Select batch 0, head h\n",
        "                x=encoder_tokens,               # Use encoder tokens for X axis\n",
        "                y=decoder_tokens,               # Use decoder tokens for Y axis\n",
        "                colorscale='Viridis',\n",
        "                name=f'Head {h}',\n",
        "                hoverongaps = False,\n",
        "                visible=(h == 0), # Only first head visible initially\n",
        "                showscale=True,   # Show color scale by default\n",
        "                colorbar=dict(title='Attention Weight', titleside='right') # Add color bar title\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "    # Create dropdown menu to select head\n",
        "    buttons = []\n",
        "    for h in range(num_heads):\n",
        "        # Create a visibility mask: True for the selected head, False for others\n",
        "        visibility = [False] * num_heads\n",
        "        visibility[h] = True\n",
        "        buttons.append(dict(\n",
        "            label=f'Head {h}',\n",
        "            method='update',\n",
        "            # Update visibility and the plot title when a head is selected\n",
        "            args=[{'visible': visibility},\n",
        "                  {'title.text': f'Cross-Attention Weights - Decoder Layer {layer_idx}, Head {h}'}]\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        updatemenus=[dict(\n",
        "            active=0,\n",
        "            buttons=buttons,\n",
        "            direction=\"down\",\n",
        "            pad={\"r\": 10, \"t\": 10},\n",
        "            showactive=True,\n",
        "            x=0.05, # Position dropdown slightly left\n",
        "            xanchor=\"left\",\n",
        "            y=1.18, # Position dropdown slightly higher\n",
        "            yanchor=\"top\"\n",
        "        )],\n",
        "        title=dict(\n",
        "            text=f'Cross-Attention Weights - Decoder Layer {layer_idx}, Head 0', # Initial title\n",
        "            x=0.5, # Center title\n",
        "            xanchor='center'\n",
        "        ),\n",
        "        xaxis_title=\"Encoder Tokens (Key / Value from Source)\",\n",
        "        yaxis_title=\"Decoder Tokens (Query from Target)\",\n",
        "        yaxis_autorange='reversed', # Puts decoder start token at the top\n",
        "        xaxis_tickangle=-45,       # Angle ticks for better readability\n",
        "        height=700,                # Adjust height if needed\n",
        "        width=850,                 # Adjust width if needed\n",
        "        xaxis_showgrid=False,      # Hide grid lines for cleaner look\n",
        "        yaxis_showgrid=False,\n",
        "        hovermode='closest',       # Show hover info for the closest point\n",
        "    )\n",
        "\n",
        "    # Update y-axis tick labels for better readability if needed\n",
        "    fig.update_yaxes(tickfont=dict(size=10))\n",
        "    fig.update_xaxes(tickfont=dict(size=10))\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "_7vqbinpJkim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction: What is a Transformer?\n",
        "\n",
        "The Transformer architecture was introduced in the paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). It revolutionized NLP by relying entirely on **attention mechanisms**, discarding recurrence (like RNNs/LSTMs) and convolutions.\n",
        "\n",
        "**Why was this revolutionary?**\n",
        "\n",
        "1.  **Parallelization:** Unlike RNNs that process sequences step-by-step, Transformers can process all tokens in a sequence simultaneously, making training much faster on modern hardware (GPUs/TPUs).\n",
        "2.  **Long-Range Dependencies:** Attention mechanisms allow the model to directly model dependencies between any two tokens in the sequence, regardless of their distance, overcoming limitations of RNNs with long sequences.\n",
        "\n",
        "**High-Level Architecture:**\n",
        "\n",
        "The original Transformer has an **Encoder-Decoder** structure, commonly used for sequence-to-sequence tasks like machine translation or summarization.\n",
        "\n",
        "> ![Transformer Architecture](https://deeprevision.github.io/posts/001-transformer/transformer.png)\n",
        "\n",
        "\n",
        "*   **Encoder:** Maps an input sequence of symbols $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$.\n",
        "*   **Decoder:** Given $\\mathbf{z}$, generates an output sequence $(y_1, ..., y_m)$ one symbol at a time (autoregressive).\n",
        "\n",
        "Many popular models are variations:\n",
        "*   **Encoder-Only:** BERT, RoBERTa, ALBERT (good for NLU tasks like classification, QA, NER).\n",
        "*   **Decoder-Only:** GPT series (good for text generation).\n",
        "*   **Encoder-Decoder:** Original Transformer, T5, BART (good for seq2seq tasks).\n",
        "\n",
        "We'll explore all the key components shown in the diagram."
      ],
      "metadata": {
        "id": "IqOy57f494cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization: Converting Text to Numbers\n",
        "\n",
        "Transformers don't understand raw text. We need to convert text into a sequence of numerical IDs, a process called **tokenization**.\n",
        "\n",
        "**Why not just use words?**\n",
        "* Vocabulary size would be enormous (millions of words, including typos, variations).\n",
        "* Handling unknown words (\"Out-Of-Vocabulary\" or OOV problem).\n",
        "\n",
        "**Common Tokenization Strategies:**\n",
        "\n",
        "1. **Word-Based:** Split by spaces/punctuation. Simple, but suffers from large vocab & OOV.\n",
        "2. **Character-Based:** Split into individual characters. Small vocab, no OOV, but loses word-level meaning and creates very long sequences.\n",
        "3. **Subword-Based:** The sweet spot! Breaks words into smaller, meaningful units. Common words remain intact, rare words are broken down. Handles OOV gracefully and keeps vocabulary size manageable.\n",
        "    * **Byte-Pair Encoding (BPE):** Starts with characters, iteratively merges most frequent pairs. Used by GPT, RoBERTa.\n",
        "        * **Byte-Level BPE (BBPE):** A variant of BPE that operates directly on the raw bytes of the input text rather than on characters. This approach enhances robustness since it can efficiently handle any Unicode string, including rare symbols and diverse languages. It is famously used in models like GPT-2.\n",
        "    * **WordPiece:** Similar to BPE, but merges pairs that maximize likelihood of training data. Used by BERT, DistilBERT.\n",
        "    * **SentencePiece:** Treats the input as a raw stream, includes whitespace in the tokens. Language-agnostic. Used by T5, XLNet.\n",
        "\n",
        "**Let's use a real tokenizer (WordPiece from BERT).**\n"
      ],
      "metadata": {
        "id": "aciYOXrR-wsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained tokenizer (bert-base-uncased)\n",
        "tokenizer_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "print(f\"Loaded Tokenizer: {tokenizer_name}\")\n",
        "print(f\"Vocabulary Size: {tokenizer.vocab_size}\")\n",
        "print(f\"Special Tokens: {tokenizer.special_tokens_map}\")\n",
        "print(f\"CLS token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
        "print(f\"SEP token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
        "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
        "\n",
        "# Example sentence\n",
        "text = \"Transformers are powerful! They revolutionized NLP.\"\n",
        "text2 = \"Let's tokenize this sentence using BertTokenizer.\"\n",
        "\n",
        "# 1. Tokenize: Split into tokens (subwords)\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"\\nText: '{text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# 2. Convert tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# 3. Add special tokens ([CLS] and [SEP]) - common practice for BERT\n",
        "# [CLS] often used for classification tasks, [SEP] separates segments.\n",
        "encoded_plus = tokenizer.encode_plus(text, add_special_tokens=True)\n",
        "special_token_ids = encoded_plus['input_ids']\n",
        "special_tokens = tokenizer.convert_ids_to_tokens(special_token_ids)\n",
        "\n",
        "print(f\"\\nTokens with Special Tokens: {special_tokens}\")\n",
        "print(f\"Token IDs with Special Tokens: {special_token_ids}\")\n",
        "print(f\"Attention Mask: {encoded_plus['attention_mask']}\") # Tells model which tokens to attend to (ignore padding)\n",
        "\n",
        "# 4. Decode: Convert IDs back to text\n",
        "decoded_text = tokenizer.decode(special_token_ids, skip_special_tokens=False)\n",
        "decoded_text_no_special = tokenizer.decode(special_token_ids, skip_special_tokens=True)\n",
        "print(f\"\\nDecoded Text (with special): {decoded_text}\")\n",
        "print(f\"Decoded Text (no special): {decoded_text_no_special}\")\n",
        "\n",
        "# Example showing subword splitting\n",
        "print(\"\\nSubword Example:\")\n",
        "print(f\"Tokenizing 'revolutionized': {tokenizer.tokenize('revolutionized')}\")\n",
        "print(f\"Tokenizing 'Tokenization': {tokenizer.tokenize('Tokenization')}\") # Note the capitalization handling for 'uncased' model"
      ],
      "metadata": {
        "id": "PzAwYQb-y7p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch encoding example (handling multiple sentences, padding, truncation)\n",
        "sentences = [text, text2]\n",
        "batch_encoded = tokenizer(\n",
        "    sentences,\n",
        "    padding=True,        # Pad shorter sequences to the length of the longest\n",
        "    truncation=True,     # Truncate sequences longer than max model length\n",
        "    max_length=20,       # Example max length\n",
        "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "print(\"\\nBatch Encoding Example:\")\n",
        "print(\"Input IDs (Batch):\\n\", batch_encoded['input_ids'])\n",
        "print(\"Attention Mask (Batch):\\n\", batch_encoded['attention_mask'])\n",
        "\n",
        "\n",
        "visualize_tokenization(\"Let's understand tokenization.\", tokenizer)\n",
        "visualize_tokenization(\"You are great students!\", tokenizer)"
      ],
      "metadata": {
        "id": "hdM6oDsdITAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Tips (Tokenization):**\n",
        "*   Always use the *exact* tokenizer that the pre-trained model was trained with. Mismatched tokenizers lead to poor performance.\n",
        "*   Pay attention to `uncased` vs `cased` models. `uncased` models lowercase text before tokenization.\n",
        "*   Understand the role of special tokens (`[CLS]`, `[SEP]`, `[PAD]`, `[UNK]`, `[MASK]`) for your specific model and task.\n",
        "*   Padding and Attention Masks are crucial when processing batches of sequences with different lengths. The attention mask tells the model to ignore padding tokens."
      ],
      "metadata": {
        "id": "DCO-3kv0_A8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embeddings and Positional Encoding\n",
        "\n",
        "Once we have token IDs, we need to convert them into dense vectors (embeddings) that the model can process.\n",
        "\n",
        "**Input Embeddings:**\n",
        "*   A simple lookup table (Embedding Matrix).\n",
        "*   Each token ID corresponds to a row in the matrix.\n",
        "*   The size of the matrix is `(vocabulary_size, embedding_dimension)`.\n",
        "*   `embedding_dimension` (often denoted `d_model`) is a hyperparameter (e.g., 768 for BERT-base).\n",
        "*   These embeddings are *learned* during training.\n",
        "\n",
        "**The Problem:** Standard Transformers have no built-in sense of sequence order (unlike RNNs). If you shuffle the input embeddings, the self-attention output would also be shuffled â€“ the meaning would be lost!\n",
        "\n",
        "**Positional Encoding (PE):**\n",
        "*   Injects information about the position of each token in the sequence.\n",
        "*   These positional encodings are *added* to the input embeddings.\n",
        "*   The original paper used fixed sine and cosine functions of different frequencies:\n",
        "    *   $PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})$\n",
        "    *   $PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})$\n",
        "    *   Where `pos` is the token position, `i` is the dimension index within the embedding vector (`0` to `d_model/2 - 1`), and `d_model` is the embedding dimension.\n",
        "\n",
        "**Why sine and cosine?**\n",
        "*   Produces unique encodings for each position.\n",
        "*   Allows the model to easily attend to relative positions, since $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\n",
        "*   Values remain bounded between -1 and 1.\n",
        "*   Can generalize to sequence lengths longer than those seen during training (though performance might degrade)."
      ],
      "metadata": {
        "id": "Ewb82rIC_KFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Learn how nn.Embedding works. What must written inside it?\n",
        "* Learn how positional encoding works. Write a final formulas for PE.\n",
        "\n",
        "**Useful links**:\n",
        "1. https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ],
      "metadata": {
        "id": "XnHy_s-SuTkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use a pre-trained model's embeddings later,\n",
        "# but here's how you'd define one conceptually:\n",
        "vocab_size = tokenizer.vocab_size # From our BERT tokenizer\n",
        "d_model = 128 # Smaller dimension for easier visualization/computation here\n",
        "embedding_layer = nn.Embedding(<YOUR_CODE>)\n",
        "\n",
        "# Example: Get embeddings for our batch_encoded IDs\n",
        "# (Using the conceptual layer, not BERT's actual embeddings yet)\n",
        "sample_ids = batch_encoded['input_ids'].detach().clone()\n",
        "sample_embeddings = embedding_layer(sample_ids)\n",
        "print(f\"Sample Token IDs shape: {sample_ids.shape}\") # (batch_size, seq_len)\n",
        "print(f\"Sample Embeddings shape: {sample_embeddings.shape}\") # (batch_size, seq_len, d_model)\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Dimension of the embeddings.\n",
        "            max_len (int): Maximum sequence length.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Position indices (0, 1, ..., max_len-1) -> shape (max_len, 1)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Term for division: 10000^(2i / d_model)\n",
        "        # Calculate 2i first: torch.arange(0, d_model, 2) -> (0, 2, ..., d_model-2)\n",
        "        # Then calculate the exponent term\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Calculate sine for even indices, cosine for odd indices\n",
        "        pe[:, 0::2] = <YOUR_CODE> # Even indices\n",
        "        pe[:, 1::2] = <YOUR_CODE> # Odd indices\n",
        "\n",
        "        # Add a batch dimension (1, max_len, d_model) so it can be added to input embeddings\n",
        "        # Using register_buffer makes 'pe' part of the model's state, but not a parameter to be trained\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input embeddings (batch_size, seq_len, d_model).\n",
        "        Returns:\n",
        "            torch.Tensor: Embeddings with added positional encoding.\n",
        "        \"\"\"\n",
        "        # x.size(1) is the sequence length of the current batch\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # self.pe is (1, max_len, d_model), slice it to match input seq_len\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x) # Apply dropout\n",
        "\n",
        "\n",
        "# Instantiate Positional Encoding\n",
        "max_sequence_length = 50 # Max length for our example\n",
        "pe_layer = PositionalEncoding(d_model, max_len=max_sequence_length)\n",
        "\n",
        "# Apply PE to our sample embeddings\n",
        "embeddings_with_pe = pe_layer(sample_embeddings)\n",
        "print(f\"\\nEmbeddings with PE shape: {embeddings_with_pe.shape}\")\n",
        "\n",
        "# --- Visualize Positional Encoding ---\n",
        "def visualize_pe(pe_layer, max_len_to_show=50, d_model_to_show=128):\n",
        "    pe = pe_layer.pe.squeeze(0).cpu().numpy() # Remove batch dim, move to CPU, convert to numpy\n",
        "    pe = pe[:max_len_to_show, :d_model_to_show] # Slice for visualization\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pe, cmap=\"viridis\")\n",
        "    plt.xlabel(\"Embedding Dimension Index\")\n",
        "    plt.ylabel(\"Token Position in Sequence\")\n",
        "    plt.title(f\"Positional Encoding (First {max_len_to_show} Positions, {d_model_to_show} Dimensions)\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_pe(pe_layer, max_len_to_show=max_sequence_length, d_model_to_show=d_model)\n",
        "\n",
        "print(\"\\nObservations from PE visualization:\")\n",
        "print(\"- Each row (position) has a unique encoding pattern.\")\n",
        "print(\"- Columns (dimensions) vary with different frequencies (sine/cosine waves).\")\n",
        "print(\"- Smooth transitions between positions, potentially allowing generalization.\")"
      ],
      "metadata": {
        "id": "nGvSolxmzIMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Combine Embeddings and PE using a real model (BERT) ---\n",
        "# Load a small BERT model\n",
        "model_name = \"bert-base-uncased\"\n",
        "bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "bert_embeddings = bert_model.embeddings # Access the embedding module\n",
        "\n",
        "# Prepare input for BERT\n",
        "text_for_bert = \"Example sentence for BERT embeddings.\"\n",
        "inputs = tokenizer(text_for_bert, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Get embeddings from BERT (includes token, position, and token_type embeddings added together)\n",
        "bert_model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    outputs = bert_model(**inputs, output_hidden_states=True)\n",
        "    final_embeddings = outputs.hidden_states[0] # Embeddings are the first hidden state\n",
        "\n",
        "print(f\"\\n--- BERT Embeddings ---\")\n",
        "print(f\"Input text: '{text_for_bert}'\")\n",
        "print(f\"Input IDs shape: {input_ids.shape}\")\n",
        "print(f\"BERT Final Embeddings (Layer 0 output) shape: {final_embeddings.shape}\") # (batch_size, seq_len, 768)\n",
        "\n",
        "# Let's visualize the components of BERT's embeddings\n",
        "bert_embedding_dim = bert_model.config.hidden_size # Should be 768 for base\n",
        "token_embed = bert_embeddings.word_embeddings(input_ids)\n",
        "pos_embed = bert_embeddings.position_embeddings(torch.arange(input_ids.shape[1], device=device).unsqueeze(0))\n",
        "# BERT also has token_type_embeddings, usually 0 for single sentences\n",
        "tok_type_ids = torch.zeros_like(input_ids, device=device)\n",
        "tok_type_embed = bert_embeddings.token_type_embeddings(tok_type_ids)\n",
        "\n",
        "summed_embeds = token_embed + pos_embed + tok_type_embed\n",
        "# LayerNorm is applied after summing in BERT's embedding layer\n",
        "normed_embeds = bert_embeddings.LayerNorm(summed_embeds)\n",
        "# Dropout is also applied\n",
        "final_embeds_manual = bert_embeddings.dropout(normed_embeds)\n",
        "\n",
        "# Check if manually calculated embeddings match the model's output\n",
        "# Use a tolerance due to potential floating point differences\n",
        "print(f\"Manually calculated embeddings match model output[0]? {torch.allclose(final_embeds_manual, final_embeddings, atol=1e-6)}\")\n",
        "\n",
        "# Visualize BERT's Positional Embeddings (different from the sin/cos version)\n",
        "# BERT uses learned positional embeddings\n",
        "bert_pos_embed_weights = bert_embeddings.position_embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Only show first N positions for clarity\n",
        "num_pos_to_show = 50\n",
        "sns.heatmap(bert_pos_embed_weights[:num_pos_to_show, :], cmap=\"viridis\")\n",
        "plt.xlabel(\"Embedding Dimension Index\")\n",
        "plt.ylabel(\"Token Position in Sequence\")\n",
        "plt.title(f\"BERT Learned Positional Embeddings (First {num_pos_to_show} Positions)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: BERT uses *learned* positional embeddings, not the fixed sin/cos ones.\")\n",
        "print(\"The pattern looks different, but serves the same purpose: encoding position.\")"
      ],
      "metadata": {
        "id": "xRhH0cPJ_VuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Tips (Embeddings & PE):**\n",
        "*   The `d_model` (embedding dimension) must be consistent throughout the Transformer layers.\n",
        "*   While the original paper used fixed sin/cos PE, many modern Transformers (like BERT) use *learned* positional embeddings, which are just another embedding layer indexed by position.\n",
        "*   Adding PE is crucial. Without it, the model is permutation-invariant.\n",
        "*   Dropout is typically applied after adding PE.\n",
        "*   BERT includes a third type of embedding: Segment Embeddings (or Token Type Embeddings), used to distinguish between different sentences in input pairs (e.g., for Next Sentence Prediction or Question Answering). We saw this as `token_type_embeddings` above."
      ],
      "metadata": {
        "id": "iSzXbUSeABHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. The Encoder: Processing the Input\n",
        "\n",
        "The Encoder's job is to take the sequence of input embeddings (with PE) and produce a sequence of contextualized representations. It consists of a stack of identical layers (N layers, e.g., N=6 for the original Transformer, N=12 for BERT-base).\n",
        "\n",
        "**Each Encoder Layer has two main sub-layers:**\n",
        "\n",
        "1.  **Multi-Head Self-Attention (MHA):** Allows each token to attend to all other tokens in the *input* sequence (including itself) to capture contextual information.\n",
        "2.  **Position-wise Feed-Forward Network (FFN):** A simple fully connected feed-forward network applied independently to each position.\n",
        "\n",
        "Residual connections (`Add`) and Layer Normalization (`Norm`) are used around each sub-layer.\n",
        "\n",
        "**Encoder Layer Structure:**\n",
        "> ![Transformer Architecture](https://www.researchgate.net/profile/Vittorio-Mazzia/publication/352992757/figure/fig1/AS:1042115790389249@1625471174152/Transformer-encoder-layer-architecture-left-and-schematic-overview-of-a-multi-head.ppm)\n",
        "\n",
        "**Let's break down Multi-Head Self-Attention first.**"
      ],
      "metadata": {
        "id": "aC-dxBUfAVdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Multi-Head Self-Attention (MHA)\n",
        "\n",
        "**Self-Attention Core Idea:** For each token, we want to compute a representation that is a weighted sum of the representations of *all* tokens in the sequence. The weights determine \"how much attention\" one token should pay to another when representing itself.\n",
        "\n",
        "**Scaled Dot-Product Attention:** This is the building block.\n",
        "\n",
        "1.  **Project Embeddings:** Create three vectors for each input embedding vector $x_i$:\n",
        "    *   **Query ($q_i$):** Represents the current token \"asking\" for information. $q_i = W_q x_i$\n",
        "    *   **Key ($k_j$):** Represents the token $x_j$ \"offering\" information or being indexed. $k_j = W_k x_j$\n",
        "    *   **Value ($v_j$):** Represents the actual content/representation of token $x_j$. $v_j = W_v x_j$\n",
        "    (Where $W_q, W_k, W_v$ are learned weight matrices).\n",
        "\n",
        "2.  **Calculate Attention Scores:** Compute the dot product between the query of the current token ($q_i$) and the keys of all tokens ($k_j$). This measures compatibility/similarity.\n",
        "    *   $score_{ij} = q_i \\cdot k_j$\n",
        "\n",
        "3.  **Scale Scores:** Divide the scores by the square root of the dimension of the key vectors ($d_k$). This prevents the dot products from growing too large for high dimensions, which could saturate the softmax function.\n",
        "    *   $scaled\\_score_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}$\n",
        "\n",
        "4.  **Apply Softmax:** Normalize the scores across all source tokens ($j$) so they sum to 1, yielding the attention weights ($\\alpha_{ij}$).\n",
        "    *   $\\alpha_{ij} = \\text{softmax}_j(scaled\\_score_{ij}) = \\frac{\\exp(scaled\\_score_{ij})}{\\sum_{l=1}^{n} \\exp(scaled\\_score_{il})}$\n",
        "\n",
        "5.  **Compute Weighted Sum:** Multiply the attention weights ($\\alpha_{ij}$) by the corresponding value vectors ($v_j$) and sum them up. This gives the output representation $z_i$ for token $i$.\n",
        "    *   $z_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j$\n",
        "\n",
        "**In Matrix Form (for the whole sequence):**\n",
        "Input $X$ (batch_size, seq_len, d_model)\n",
        "Queries $Q = X W_q$\n",
        "Keys $K = X W_k$\n",
        "Values $V = X W_v$\n",
        "(Where $W_q, W_k, W_v$ are projection matrices, shapes like (d_model, d_k) or (d_model, d_v))\n",
        "\n",
        "Attention Output $Z = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
        "\n",
        "**Multi-Head Attention:** Instead of just one set of Q, K, V projections, MHA uses multiple \"heads\".\n",
        "\n",
        "1.  Project $X$ into $h$ sets of $(Q_i, K_i, V_i)$ using different weight matrices ($W_{q,i}, W_{k,i}, W_{v,i}$) for each head $i=1...h$. The dimensions are typically $d_k = d_v = d_{model} / h$.\n",
        "2.  Perform Scaled Dot-Product Attention independently for each head, producing $h$ output vectors $Z_i$.\n",
        "    *   $head_i = \\text{Attention}(Q W_{q,i}, K W_{k,i}, V W_{v,i})$\n",
        "3.  Concatenate the outputs from all heads: $Concat(head_1, ..., head_h)$.\n",
        "4.  Apply a final linear projection ($W_o$) to the concatenated output to get the final MHA result.\n",
        "    *   $\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h) W_o$\n",
        "\n",
        "**Why multiple heads?** Allows the model to jointly attend to information from different representation subspaces at different positions. A single head might average away important details; multiple heads capture different types of relationships.\n",
        "\n",
        "**Let's implement Scaled Dot-Product Attention and MHA from scratch.**"
      ],
      "metadata": {
        "id": "EeiZ0yX3Aubk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Learn how Attention works. Try to understand formula and fill in the blanks."
      ],
      "metadata": {
        "id": "L5XLayyIv09g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate scaled dot product attention scores.\n",
        "\n",
        "    Args:\n",
        "        q (torch.Tensor): Queries. Shape: (batch_size, n_heads, seq_len_q, d_k)\n",
        "        k (torch.Tensor): Keys. Shape: (batch_size, n_heads, seq_len_k, d_k)\n",
        "        v (torch.Tensor): Values. Shape: (batch_size, n_heads, seq_len_v, d_v)\n",
        "                          Usually seq_len_k = seq_len_v\n",
        "        mask (torch.Tensor, optional): Mask to apply (e.g., for padding or look-ahead).\n",
        "                                       Shape should be broadcastable to (batch_size, n_heads, seq_len_q, seq_len_k).\n",
        "                                       Mask values should be 0 for tokens to attend to, and -inf (or large negative) for masked tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after attention. Shape: (batch_size, n_heads, seq_len_q, d_v)\n",
        "        torch.Tensor: Attention weights. Shape: (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    d_k = k.size(-1) # Dimension of keys\n",
        "\n",
        "    # MatMul Q and K transpose -> (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "    scores = <YOUR_CODE>\n",
        "\n",
        "    # Apply mask if provided (we will use it in next sections, just wait a bit)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9) # Use large negative value\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attn_weights = <YOUR_CODE> # Softmax over the key sequence length dimension\n",
        "\n",
        "    # MatMul attention weights and V -> (batch_size, n_heads, seq_len_q, d_v)\n",
        "    output = <YOUR_CODE>\n",
        "\n",
        "    return output, attn_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Total dimension of the model.\n",
        "            n_heads (int): Number of attention heads. d_model must be divisible by n_heads.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads # Dimension of each head's key/query/value\n",
        "\n",
        "        # Linear layers for Q, K, V projections (can be combined for efficiency)\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Projects input to Q space for all heads combined\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Projects input to K space\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Projects input to V space\n",
        "\n",
        "        # Output projection layer\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (n_heads, d_k).\n",
        "        Transpose to shape (batch_size, n_heads, seq_len, d_k).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
        "            batch_size (int): Batch size.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor reshaped for multi-head attention.\n",
        "                          Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.n_heads, self.d_k) # (batch_size, seq_len, n_heads, d_k)\n",
        "        return x.transpose(1, 2) # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Multi-Head Attention.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query input. Shape: (batch_size, seq_len_q, d_model)\n",
        "            key (torch.Tensor): Key input. Shape: (batch_size, seq_len_k, d_model)\n",
        "            value (torch.Tensor): Value input. Shape: (batch_size, seq_len_v, d_model)\n",
        "                                  Usually seq_len_k = seq_len_v\n",
        "            mask (torch.Tensor, optional): Mask. Shape broadcastable to (batch_size, 1, seq_len_q, seq_len_k).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after MHA. Shape: (batch_size, seq_len_q, d_model)\n",
        "            torch.Tensor: Attention weights. Shape: (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1. Project Q, K, V using linear layers\n",
        "        q = <YOUR_CODE> # (batch_size, seq_len_q, d_model)\n",
        "        k = <YOUR_CODE>  # (batch_size, seq_len_k, d_model)\n",
        "        v = <YOUR_CODE>   # (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        # 2. Split into multiple heads\n",
        "        q = self.split_heads(q, batch_size) # (batch_size, n_heads, seq_len_q, d_k)\n",
        "        k = self.split_heads(k, batch_size) # (batch_size, n_heads, seq_len_k, d_k)\n",
        "        v = self.split_heads(v, batch_size) # (batch_size, n_heads, seq_len_v, d_k)\n",
        "\n",
        "        # 3. Apply scaled dot-product attention\n",
        "        # The mask needs to be compatible: (batch_size, 1, seq_len_q, seq_len_k)\n",
        "        # or similar broadcastable shape.\n",
        "        attention_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # attention_output shape: (batch_size, n_heads, seq_len_q, d_k)\n",
        "        # attn_weights shape: (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # 4. Concatenate heads and project back\n",
        "        # Transpose back to (batch_size, seq_len_q, n_heads, d_k)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
        "        # Reshape to (batch_size, seq_len_q, d_model)\n",
        "        concat_attention = attention_output.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Apply final linear layer W_o\n",
        "        output = self.W_o(concat_attention) # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attn_weights"
      ],
      "metadata": {
        "id": "A_zdOUDSy7td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy input data\n",
        "batch_size = 1\n",
        "seq_len = 5\n",
        "d_model = 128\n",
        "n_heads = 8\n",
        "\n",
        "dummy_input = torch.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# Instantiate MHA layer\n",
        "mha_layer = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "# In self-attention, query, key, and value are the same\n",
        "# No mask needed for this simple example yet\n",
        "output, attn_weights = mha_layer(dummy_input, dummy_input, dummy_input, mask=None)\n",
        "\n",
        "print(\"--- MHA Scratch Implementation ---\")\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\") # (batch_size, n_heads, seq_len, seq_len)"
      ],
      "metadata": {
        "id": "uscVglfoI6yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Visualizing Self-Attention\n",
        "\n",
        "Now, let's use a pre-trained BERT model to visualize the *actual* attention weights on a real sentence. This shows which words the model focuses on when representing a specific word.\n",
        "\n",
        "We'll extract the attention weights from one of BERT's encoder layers. BERT-base has 12 layers and 12 heads per layer."
      ],
      "metadata": {
        "id": "kuMj_Bn4A_oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Analyze attention layers, which words are more \"connected\" with each other in your chosen layer."
      ],
      "metadata": {
        "id": "qMIg1xY1w1bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload BERT model if needed, ensuring we get attention outputs\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# Choose a sentence where attention patterns might be interesting\n",
        "# text = \"The cat sat on the mat, because it was tired.\" # \"it\" should attend to \"cat\" or \"mat\"?\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "# text = \"I went to the bank to deposit money.\" # \"bank\" has multiple meanings\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "token_list = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "print(f\"Input Text: '{text}'\")\n",
        "print(f\"Tokens: {token_list}\")\n",
        "\n",
        "# Perform inference and get outputs, including attentions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions # Tuple of tensors, one for each layer\n",
        "    # Each tensor shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
        "\n",
        "# Let's examine attentions from a specific layer\n",
        "layer_index = <YOUR_CODE> # You can choose first layer (0) or any other :)\n",
        "attention_layer = attentions[layer_index].cpu() # Move to CPU for plotting\n",
        "# Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "# Average attention weights across all heads for a simpler view first\n",
        "attention_avg_heads = attention_layer.mean(dim=1).squeeze(0) # Squeeze batch dim\n",
        "# Shape: (seq_len, seq_len)\n",
        "\n",
        "# --- Plotting Function ---\n",
        "def plot_attention_heatmap(attention_matrix, x_labels, y_labels, title):\n",
        "    \"\"\"Plots a heatmap for attention weights.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(attention_matrix, xticklabels=x_labels, yticklabels=y_labels, cmap='viridis', linewidths=.1)\n",
        "    plt.xlabel(\"Key (Attended To)\")\n",
        "    plt.ylabel(\"Query (Attending From)\")\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot average attention weights for layer 0\n",
        "plot_attention_heatmap(attention_avg_heads.numpy(), token_list, token_list, f\"Average Self-Attention (Layer {layer_index})\")"
      ],
      "metadata": {
        "id": "gHVva07Ay7xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot interactive heatmap for layer 0\n",
        "print(\"\\n--- Interactive Self-Attention Plot ---\")\n",
        "plot_attention_interactive(attention_layer, token_list, layer_index)\n",
        "\n",
        "# It is interactive, so you can just click on plot to choose head"
      ],
      "metadata": {
        "id": "DczW32eDJPiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting Self-Attention Visualizations:**\n",
        "\n",
        "*   **Diagonal:** Tokens usually attend strongly to themselves (check Head 6).\n",
        "*   **Off-Diagonal Bright Spots:** Indicate strong attention between different tokens. Look for meaningful relationships (e.g., pronouns attending to nouns, verbs attending to subjects/objects).\n",
        "*   **`[CLS]` Token:** Often aggregates information from the entire sequence, especially in later layers, as it's used for classification tasks.\n",
        "*   **`[SEP]` Token:** Marks boundaries; attention patterns around it can be interesting.\n",
        "*   **Padding Tokens (`[PAD]`):** Should have near-zero attention weights directed *towards* them if the attention mask is working correctly (though they might attend *to* other tokens).\n",
        "*   **Different Heads, Different Patterns:** Notice how different heads capture different relationships (e.g., some might focus on local context, others on syntactic dependencies, others on distant relationships). This highlights the benefit of MHA.\n",
        "\n",
        "**Practical Tips (Self-Attention):**\n",
        "*   The dimensionality of Q, K, and V doesn't *have* to be the same, but it often is ($d_k = d_v = d_{model} / n_{heads}$).\n",
        "*   Scaling by $\\sqrt{d_k}$ is crucial for stable training.\n",
        "*   Attention masks are essential for handling padding and for decoder self-attention (look-ahead mask)."
      ],
      "metadata": {
        "id": "5DU-Ks5MBH39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Add & Norm and Feed-Forward Network\n",
        "\n",
        "**Add & Norm:**\n",
        "\n",
        "*   **Residual Connection (`Add`):** The input to the sub-layer is added to the output of the sub-layer ($x + \\text{Sublayer}(x)$).\n",
        "    *   **Why?** Helps gradients flow during training (mitigates vanishing gradients), allows layers to learn modifications to the identity function, leading to deeper networks.\n",
        "*   **Layer Normalization (`Norm`):** Normalizes the activations *across the feature dimension* for each token independently.\n",
        "    *   $LN(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n",
        "    *   Where $\\mu$ and $\\sigma$ are the mean and standard deviation calculated over the `d_model` dimension for a specific token, and $\\gamma$ (gamma) and $\\beta$ (beta) are learnable scale and shift parameters.\n",
        "    *   **Why Layer Norm (vs Batch Norm)?** Works well for variable sequence lengths and performs consistently across batch sizes, which is common in NLP. Stabilizes activations and improves training.\n",
        "\n",
        "**Position-wise Feed-Forward Network (FFN):**\n",
        "\n",
        "*   A simple network applied independently to each position (each token's representation) in the sequence.\n",
        "*   Consists of two linear transformations with a non-linearity (usually ReLU or GELU) in between.\n",
        "    *   $\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2$ (using ReLU)\n",
        "    *   The inner dimension (`d_ff`, typically $4 \\times d_{model}$) is larger than the input/output dimension (`d_model`).\n",
        "*   **Why?** Adds non-linearity and capacity to the model, allowing it to learn more complex transformations of the token representations after the attention mechanism has mixed information across the sequence. It can be seen as processing the information aggregated by the attention layer for each token.\n",
        "\n",
        "**Let's implement the FFN and put together a full Encoder Layer.**"
      ],
      "metadata": {
        "id": "XDkmLVUcBdeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Try to realize forward pass in PositionwiseFeedForward and EncoderLayer."
      ],
      "metadata": {
        "id": "Taew3wTZxObA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Input and output dimension.\n",
        "            d_ff (int): Inner dimension (usually 4 * d_model).\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        # Common activation functions: ReLU or GELU (used in BERT)\n",
        "        # self.activation = nn.ReLU()\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (batch_size, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        # Lin1->activation->dropour->lin2\n",
        "        <YOUR_CODE>\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Model dimension.\n",
        "            n_heads (int): Number of attention heads.\n",
        "            d_ff (int): Inner dimension of FFN.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6) # Epsilon for numerical stability\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for a single Encoder layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
        "            mask (torch.Tensor): Attention mask (broadcastable to batch_size, 1, seq_len, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (batch_size, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        # 1. Multi-Head Self-Attention + Add & Norm\n",
        "        <YOUR_CODE> # Self-attention: Q, K, V are all x\n",
        "        # Apply dropout to attention output, then add residual connection, then layer norm\n",
        "        <YOUR_CODE>\n",
        "\n",
        "        # 2. Feed-Forward Network + Add & Norm\n",
        "        <YOUR_CODE>\n",
        "        # Apply dropout to FFN output, then add residual connection, then layer norm\n",
        "        <YOUR_CODE>\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "goqlBb2ey705"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 10\n",
        "d_model = 128 # Should match MHA's d_model\n",
        "n_heads = 8   # Should match MHA's n_heads\n",
        "d_ff = d_model * 4 # Common practice\n",
        "\n",
        "# Dummy input and mask\n",
        "dummy_input = torch.rand(batch_size, seq_len, d_model)\n",
        "# Example padding mask: assume last 3 tokens are padding\n",
        "# Mask needs shape compatible with MHA: (batch_size, 1, seq_len_q, seq_len_k)\n",
        "# Here seq_len_q = seq_len_k = seq_len\n",
        "dummy_mask = torch.ones(batch_size, 1, 1, seq_len) # Start with all ones (attend)\n",
        "dummy_mask[:, :, :, -3:] = 0 # Mask out last 3 tokens (0 means mask)\n",
        "# Ensure mask is on the correct device if using GPU\n",
        "dummy_mask = dummy_mask.to(dummy_input.device)\n",
        "\n",
        "# Instantiate EncoderLayer\n",
        "encoder_layer = EncoderLayer(d_model, n_heads, d_ff)\n",
        "\n",
        "# Pass input through the layer\n",
        "encoder_output = encoder_layer(dummy_input, dummy_mask)\n",
        "\n",
        "print(\"--- Encoder Layer Scratch Implementation ---\")\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Mask shape (original): (batch_size, 1, 1, seq_len)\")\n",
        "print(f\"Output shape: {encoder_output.shape}\") # Should be same as input shape"
      ],
      "metadata": {
        "id": "o1MVv4wmJZTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Layer-wise Attention Pattern Evolution (Encoder Self-Attention)\n",
        "How do the self-attention patterns change as information propagates through the layers of the Encoder?\n",
        "\n",
        "Lower layers might focus more on local, syntactic relationships, while higher layers might capture broader, more semantic context. Let's visualize the average self-attention weights from different layers in BERT."
      ],
      "metadata": {
        "id": "48tLmjDEFkg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure BERT model and tokenizer are loaded from previous cells\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Use a sentence where context matters\n",
        "text_layer_viz = \"The animal didn't cross the street because it was too tired.\"\n",
        "# text_layer_viz = \"I went to the bank to deposit money, not the river bank.\"\n",
        "\n",
        "inputs_layer_viz = tokenizer(text_layer_viz, return_tensors=\"pt\").to(device)\n",
        "tokens_layer_viz = tokenizer.convert_ids_to_tokens(inputs_layer_viz[\"input_ids\"][0])\n",
        "\n",
        "# Get attentions from all layers\n",
        "with torch.no_grad():\n",
        "    outputs_layer_viz = model(**inputs_layer_viz)\n",
        "    attentions_all_layers = outputs_layer_viz.attentions # Tuple of (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "num_layers = len(attentions_all_layers)\n",
        "print(f\"Number of layers in {model_name}: {num_layers}\")\n",
        "\n",
        "# Select layers to visualize (e.g., first, latest)\n",
        "layers_to_show = [0, num_layers - 3]\n",
        "\n",
        "print(f\"\\nVisualizing Average Self-Attention for Layers: {layers_to_show}\")\n",
        "\n",
        "for i, layer_idx in enumerate(layers_to_show):\n",
        "    # Get attention for the specific layer, move to CPU\n",
        "    attention_layer = attentions_all_layers[layer_idx].cpu()\n",
        "    # Average across heads\n",
        "    attention_avg_heads = attention_layer.mean(dim=1).squeeze(0) # Squeeze batch dim\n",
        "    # Shape: (seq_len, seq_len)\n",
        "\n",
        "    plot_attention_heatmap_improved(\n",
        "        attention_avg_heads.numpy(),\n",
        "        x_labels=tokens_layer_viz,\n",
        "        y_labels=tokens_layer_viz,\n",
        "        title=f\"Average Self-Attention Pattern (Layer {layer_idx})\",\n",
        "        figsize=(10, 8), # Adjust size if needed\n",
        "        fmt=\".1f\" # Use fewer decimals for less clutter\n",
        "    )\n",
        "\n",
        "print(f\"\\nObserve how patterns change:\")\n",
        "print(f\"- Layer {layers_to_show[0]} (First): Often focuses on local context, diagonal, maybe syntax.\")\n",
        "print(f\"- Layer {layers_to_show[1]} (Latest): Often shows more diffused attention, potentially focusing on key semantic tokens or the [CLS] token aggregating context.\")\n",
        "print(f\"Look at the row for 'it' - which token(s) does it attend to most strongly in different layers ('animal' vs 'street')?\")"
      ],
      "metadata": {
        "id": "HwZE13hqFjhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Using Encoder Output: The [CLS] Token for Classification (Conceptual)\n",
        "For models like BERT (Encoder-only), a common way to perform sequence classification (e.g., sentiment analysis) is to use the final hidden state corresponding to the special $[CLS]$ token.\n",
        "\n",
        "This token's representation is assumed to aggregate the contextual information from the entire sequence after passing through all encoder layers. A simple linear classifier is then trained on top of this single vector."
      ],
      "metadata": {
        "id": "le-YwSuWF6Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure BERT model and tokenizer are loaded\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device) # No need for attentions here\n",
        "model.eval()\n",
        "\n",
        "# Input sentence\n",
        "text_cls = \"This movie was absolutely fantastic!\"\n",
        "inputs_cls = tokenizer(text_cls, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "print(f\"Input Text: '{text_cls}'\")\n",
        "print(f\"Input IDs: {inputs_cls['input_ids']}\")\n",
        "tokens_cls = tokenizer.convert_ids_to_tokens(inputs_cls['input_ids'][0])\n",
        "print(f\"Tokens: {tokens_cls}\")\n",
        "\n",
        "# Get the outputs from the BERT model\n",
        "with torch.no_grad():\n",
        "    outputs_cls = model(**inputs_cls)\n",
        "    # outputs_cls.last_hidden_state contains the final hidden states for all tokens\n",
        "    # Shape: (batch_size, sequence_length, hidden_size)\n",
        "    last_hidden_states = outputs_cls.last_hidden_state\n",
        "\n",
        "print(f\"\\nShape of final hidden states: {last_hidden_states.shape}\")\n",
        "\n",
        "# The [CLS] token is always the first token (index 0)\n",
        "cls_hidden_state = last_hidden_states[:, 0, :] # Select the hidden state for the [CLS] token\n",
        "# Shape: (batch_size, hidden_size)\n",
        "print(f\"Shape of [CLS] token hidden state: {cls_hidden_state.shape}\")\n",
        "print(f\"Hidden size (d_model) for {model_name}: {model.config.hidden_size}\")"
      ],
      "metadata": {
        "id": "SZPtyyT8F6Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Conceptual Classifier ---\n",
        "# In a real scenario, you would define and train this layer\n",
        "hidden_size = model.config.hidden_size\n",
        "num_classes = 2 # Example: Positive/Negative sentiment (binary)\n",
        "\n",
        "# Define a simple linear layer (untrained)\n",
        "classifier_head = nn.Linear(hidden_size, num_classes).to(device)\n",
        "\n",
        "# Pass the [CLS] token's hidden state through the classifier\n",
        "# (We're not training, just showing the mechanism)\n",
        "logits_cls = classifier_head(cls_hidden_state)\n",
        "# Shape: (batch_size, num_classes)\n",
        "print(f\"\\nShape of output logits from conceptual classifier: {logits_cls.shape}\")\n",
        "\n",
        "# These logits would then be used with a loss function (e.g., CrossEntropyLoss)\n",
        "# during training, or passed through a softmax for prediction probabilities during inference.\n",
        "print(\"Mechanism: The [CLS] token's final hidden state is used as the aggregated sequence representation for classification tasks.\")"
      ],
      "metadata": {
        "id": "FCtNuCXpyMRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to understand the code above. We will speak a lot about BERT models next Monday!"
      ],
      "metadata": {
        "id": "0zWzm7sSyNvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Tips (Encoder Layer):**\n",
        "*   The order matters: Attention -> Add & Norm -> FFN -> Add & Norm.\n",
        "*   Dropout is applied at multiple points (usually after MHA, after FFN, after PE/Embedding addition) to prevent overfitting.\n",
        "*   Layer Normalization parameters ($\\gamma, \\beta$) are learned.\n",
        "*   The choice of activation in FFN (ReLU, GELU) can impact performance. GELU is common in modern Transformers like BERT."
      ],
      "metadata": {
        "id": "irD9LVONBk-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. The Decoder: Generating the Output\n",
        "\n",
        "The Decoder's role is to generate the output sequence token by token, based on the encoded input representation ($\\mathbf{z}$) and the previously generated output tokens. It also consists of a stack of N identical layers.\n",
        "\n",
        "**Each Decoder Layer has *three* main sub-layers:**\n",
        "\n",
        "1.  **Masked Multi-Head Self-Attention:** Allows each position in the *output* sequence to attend to *previous* positions (including itself) in the output sequence. The \"Masked\" part is crucial to prevent attending to future tokens, maintaining the autoregressive property (generation depends only on past outputs).\n",
        "2.  **Multi-Head Cross-Attention:** This is where the Decoder interacts with the Encoder output. The queries ($Q$) come from the output of the previous Decoder sub-layer (Masked MHA), while the keys ($K$) and values ($V$) come from the **output of the Encoder**. This allows each output token to attend to relevant parts of the *input* sequence.\n",
        "3.  **Position-wise Feed-Forward Network (FFN):** Same structure and function as in the Encoder layer, applied to the output of the Cross-Attention sub-layer.\n",
        "\n",
        "Residual connections (`Add`) and Layer Normalization (`Norm`) are used around each of these three sub-layers.\n",
        "\n",
        "**Decoder Layer Structure:**\n",
        "> ![Transformer Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Transformer%2C_one_decoder_block.png/640px-Transformer%2C_one_decoder_block.png)\n",
        "\n",
        "\n",
        "**Let's look at the two new attention mechanisms in the Decoder.**"
      ],
      "metadata": {
        "id": "ej3WfiemBq2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Masked Multi-Head Self-Attention\n",
        "\n",
        "This is almost identical to the Encoder's self-attention, but with one critical difference: the **look-ahead mask**.\n",
        "\n",
        "**Look-Ahead Mask:**\n",
        "*   Prevents positions from attending to subsequent positions.\n",
        "*   Ensures that the prediction for position $i$ can only depend on the known outputs at positions less than $i$.\n",
        "*   Implemented by creating a mask matrix where entries corresponding to future positions (upper triangle of the attention score matrix) are set to $-\\infty$ (or a large negative number) before the softmax step.\n",
        "\n",
        "**Example Mask (for seq_len = 4):**\n",
        "> [[ 0., -inf, -inf, -inf], <- Pos 0 can only attend to Pos 0<br>\n",
        "> [ 0., 0., -inf, -inf], <- Pos 1 can attend to Pos 0, 1<br>\n",
        "> [ 0., 0., 0., -inf], <- Pos 2 can attend to Pos 0, 1, 2<br>\n",
        "> [ 0., 0., 0., 0.]] <- Pos 3 can attend to Pos 0, 1, 2, 3<br>\n",
        "> (Where 0 allows attention, -inf prevents it)\n",
        "\n",
        "*Note: Depending on implementation, the mask might use 1s and 0s, where 0 means \"mask out\". Our `scaled_dot_product_attention` expects 0 for masking in the `masked_fill` function.*\n",
        "\n",
        "We can modify our `scaled_dot_product_attention` function or pass the appropriate mask to our existing `MultiHeadAttention` module."
      ],
      "metadata": {
        "id": "bmRNRyeRCdu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Realize a mask creating function.\n",
        "\n",
        "**Useful functions**:\n",
        "1. https://pytorch.org/docs/stable/generated/torch.triu.html\n",
        "2. https://pytorch.org/docs/stable/generated/torch.ones.html\n"
      ],
      "metadata": {
        "id": "4S1mBbcPyim3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a look-ahead mask\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Creates a look-ahead mask for self-attention.\n",
        "    Mask has 1s where attention is allowed, 0s where it is blocked.\n",
        "    Shape: (1, 1, size, size) to be broadcastable.\n",
        "    \"\"\"\n",
        "    mask = <YOUR_CODE> # Upper triangle (True where we want to mask)\n",
        "    # In our attention function, we use mask == 0 for masking, so we invert\n",
        "    return ~mask # Lower triangle + diagonal = True (allow attention)"
      ],
      "metadata": {
        "id": "NH5YZQ3qy74h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len_dec = 6\n",
        "d_model_dec = 128\n",
        "n_heads_dec = 8\n",
        "\n",
        "# Dummy decoder input\n",
        "dummy_decoder_input = torch.rand(1, seq_len_dec, d_model_dec)\n",
        "\n",
        "# Create the look-ahead mask\n",
        "look_ahead_mask = create_look_ahead_mask(seq_len_dec)\n",
        "print(\"--- Look-Ahead Mask ---\")\n",
        "# Print the mask content (True/1 means allow, False/0 means mask)\n",
        "print(look_ahead_mask.squeeze().int())\n",
        "\n",
        "# Instantiate MHA layer\n",
        "decoder_self_mha = MultiHeadAttention(d_model_dec, n_heads_dec)\n",
        "\n",
        "# Apply masked self-attention\n",
        "masked_attn_output, masked_attn_weights = decoder_self_mha(\n",
        "    dummy_decoder_input, dummy_decoder_input, dummy_decoder_input,\n",
        "    mask=look_ahead_mask # Pass the look-ahead mask\n",
        ")\n",
        "\n",
        "print(\"\\n--- Masked Self-Attention Output ---\")\n",
        "print(f\"Decoder Input shape: {dummy_decoder_input.shape}\")\n",
        "print(f\"Masked Attention Output shape: {masked_attn_output.shape}\")\n",
        "print(f\"Masked Attention Weights shape: {masked_attn_weights.shape}\")"
      ],
      "metadata": {
        "id": "43YmcpH8J6ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the weights from our dummy example here.\n",
        "# For real models, we'd extract from a decoder layer.\n",
        "\n",
        "# Average weights across heads for visualization\n",
        "avg_masked_weights = masked_attn_weights.mean(dim=1).squeeze(0).detach().numpy()\n",
        "\n",
        "# Generate labels for the plot\n",
        "dec_labels = [f\"Dec_{i}\" for i in range(seq_len_dec)]\n",
        "\n",
        "plot_attention_heatmap(avg_masked_weights, dec_labels, dec_labels, \"Average Masked Self-Attention (Decoder)\")\n",
        "\n",
        "print(\"\\nObservations from Masked Self-Attention visualization:\")\n",
        "print(\"- The upper triangle (above the main diagonal) should have near-zero attention weights due to the mask.\")\n",
        "print(\"- Each token can only attend to itself and preceding tokens in the sequence.\")"
      ],
      "metadata": {
        "id": "uRSDSTLZJ8b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Multi-Head Cross-Attention\n",
        "\n",
        "This is the mechanism that allows the Decoder to incorporate information from the Encoder's output.\n",
        "\n",
        "*   **Queries ($Q$):** Come from the Decoder's previous sub-layer (output of the masked self-attention + Add & Norm). Shape: `(batch_size, target_seq_len, d_model)`.\n",
        "*   **Keys ($K$) and Values ($V$):** Come from the **final output of the Encoder stack**. They are the same for every step of the Decoder. Shape: `(batch_size, source_seq_len, d_model)`.\n",
        "\n",
        "The `MultiHeadAttention` module we implemented earlier can be used directly, just by feeding the correct inputs:\n",
        "`cross_attn_output, cross_attn_weights = mha_layer(query=decoder_intermediate_output, key=encoder_output, value=encoder_output, mask=padding_mask)`\n",
        "\n",
        "**Important Masking Note:** In cross-attention, the mask used should typically be the **padding mask** from the *Encoder's input* sequence. This prevents the Decoder from attending to padding tokens in the source sequence. The look-ahead mask is *not* used here because the Decoder is allowed (and encouraged) to attend to *any* position in the encoded input sequence.\n",
        "\n",
        "**Let's visualize Cross-Attention using a pre-trained Encoder-Decoder model (like T5).**"
      ],
      "metadata": {
        "id": "-PKE8a8QC2Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a small T5 model and tokenizer\n",
        "model_name_t5 = \"t5-small\"\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name_t5)\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name_t5, output_attentions=True).to(device)\n",
        "model_t5.eval()\n",
        "\n",
        "# Example: English to French translation task\n",
        "task_prefix = \"translate English to French: \"\n",
        "# Let's use a slightly longer sentence to better see attention patterns\n",
        "input_text = \"This framework is very useful for NLP tasks.\"\n",
        "target_text = \"Ce framework est trÃ¨s utile pour les tÃ¢ches NLP.\" # Corresponding French translation\n",
        "\n",
        "# Encode the input (English)\n",
        "encoder_inputs = tokenizer_t5(task_prefix + input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "encoder_input_ids = encoder_inputs['input_ids']\n",
        "# Get tokens, handling potential special characters like ' ' for SentencePiece\n",
        "encoder_tokens = [tokenizer_t5.decode([t_id]) for t_id in encoder_input_ids[0]]\n",
        "\n",
        "\n",
        "# Encode the target (French) - simulating decoder input during training/teacher forcing\n",
        "decoder_input_ids_raw = tokenizer_t5(target_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "# T5 requires decoder input IDs to start with a pad token ID for generation.\n",
        "# During training/teacher forcing, we shift the target sequence right and add the start token.\n",
        "decoder_input_ids = model_t5._shift_right(decoder_input_ids_raw).to(device)\n",
        "# Get tokens for the decoder input sequence\n",
        "decoder_tokens = [tokenizer_t5.decode([t_id]) for t_id in decoder_input_ids[0]]\n",
        "\n",
        "\n",
        "print(f\"Encoder Input Text: '{task_prefix + input_text}'\")\n",
        "print(f\"Encoder Tokens: {encoder_tokens}\")\n",
        "print(f\"Decoder Input Text (simulated): '{target_text}'\")\n",
        "print(f\"Decoder Input Tokens: {decoder_tokens}\") # Includes start token '<pad>', ends with '</s>'"
      ],
      "metadata": {
        "id": "jqx5evtMy774"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Perform Inference and Get Attentions ---\n",
        "with torch.no_grad():\n",
        "    outputs_t5 = model_t5(\n",
        "        input_ids=encoder_input_ids,\n",
        "        decoder_input_ids=decoder_input_ids, # Provide decoder inputs for teacher forcing / attention viz\n",
        "        output_attentions=True,\n",
        "        return_dict=True\n",
        "    )\n",
        "    # Cross attentions are present in the 'cross_attentions' field of the output object\n",
        "    cross_attentions = outputs_t5.cross_attentions # Tuple of tensors, one for each decoder layer\n",
        "\n",
        "# --- Visualize Cross-Attention ---\n",
        "\n",
        "# Select a decoder layer to visualize (e.g., layer 0 or a middle layer like 3)\n",
        "layer_index_dec = 3 # Try changing this index (0 to 5 for t5-small)\n",
        "cross_attention_layer = cross_attentions[layer_index_dec].cpu()\n",
        "# Shape: (batch_size, num_heads, target_sequence_length, source_sequence_length)\n",
        "\n",
        "# Average across heads for the static plot\n",
        "cross_attention_avg = cross_attention_layer.mean(dim=1).squeeze(0) # Squeeze batch dim\n",
        "# Shape: (target_seq_len, source_seq_len)\n",
        "\n",
        "print(f\"\\n--- Static Cross-Attention Plot (Layer {layer_index_dec}, Averaged Heads) ---\")\n",
        "plot_attention_heatmap_improved(\n",
        "    cross_attention_avg.numpy(),\n",
        "    x_labels=encoder_tokens,  # Source tokens (English)\n",
        "    y_labels=decoder_tokens,  # Target tokens (French)\n",
        "    title=f\"Average Cross-Attention (Decoder Layer {layer_index_dec})\"\n",
        ")"
      ],
      "metadata": {
        "id": "gh6H72NTKbnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Interactive Cross-Attention Plot (Layer {layer_index_dec}) ---\")\n",
        "# Pass the full layer tensor (with heads) to the interactive function\n",
        "plot_cross_attention_interactive(\n",
        "    cross_attention_layer,\n",
        "    encoder_tokens,\n",
        "    decoder_tokens,\n",
        "    layer_index_dec\n",
        ")"
      ],
      "metadata": {
        "id": "u5FSkl4WM9Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting Cross-Attention Visualizations:**\n",
        "\n",
        "*   Each row corresponds to a token being generated by the Decoder (Query).\n",
        "*   Each column corresponds to a token in the original input sequence processed by the Encoder (Key/Value).\n",
        "*   Bright spots indicate which input tokens the Decoder paid most attention to when generating a specific output token.\n",
        "*   You can often see alignments between source and target words (e.g., \"house\" in English attending strongly to \"maison\" in French).\n",
        "*   The `</s>` (end-of-sentence) token in the Decoder might attend broadly or to specific concluding words in the input.\n",
        "\n",
        "**Practical Tips (Decoder):**\n",
        "*   The two masks (look-ahead for self-attention, padding for cross-attention) are critical and distinct.\n",
        "*   During inference (actual generation), the Decoder operates autoregressively: generate one token, feed it back as input for the next step, repeat until an end-of-sequence token is produced.\n",
        "*   Teacher Forcing (used during training): Feed the *actual* ground-truth target sequence token as input at each step, regardless of what the model predicted previously. This stabilizes training. Our T5 visualization simulated this."
      ],
      "metadata": {
        "id": "olufmMOfC-_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Visualizing Attention Masking Effects\n",
        "We've discussed padding masks (for Encoder self-attention and Cross-attention) and look-ahead masks (for Decoder self-attention). Let's explicitly visualize how these masks affect the attention scores before the softmax step and the final attention weights after softmax.\n",
        "\n",
        "This helps understand why masking is crucial. Remember, the goal of masking is to prevent attention to certain tokens by setting their pre-softmax scores to a very large negative number (like -infinity), which results in near-zero probability after softmax."
      ],
      "metadata": {
        "id": "NgAsrHxND1GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup Dummy Data ---\n",
        "batch_size = 1\n",
        "n_heads = 1 # Simulate single head for clarity\n",
        "seq_len = 6\n",
        "d_k = 8 # Small dimension for easier inspection\n",
        "\n",
        "# Dummy Q, K, V (normally derived from projections)\n",
        "q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
        "k = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
        "v = torch.randn(batch_size, n_heads, seq_len, d_k) # d_v = d_k here\n",
        "\n",
        "# --- 1. Padding Mask Example ---\n",
        "# Assume last 2 tokens are padding\n",
        "# Mask shape (batch_size, n_heads, seq_len_q, seq_len_k) -> (1, 1, 6, 6)\n",
        "# Mask should be 1 (True) for tokens to *keep*, 0 (False) for tokens to *mask out*\n",
        "padding_mask = torch.ones(batch_size, n_heads, seq_len, seq_len)\n",
        "padding_mask[:, :, :, -2:] = 0 # Mask attention TO the last 2 key tokens\n",
        "\n",
        "# Calculate scores without mask\n",
        "scores_unmasked = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "# Calculate scores WITH mask applied *before* softmax\n",
        "scores_masked_padding = scores_unmasked.masked_fill(padding_mask == 0, -1e9)\n",
        "\n",
        "# Calculate attention weights (softmax)\n",
        "attn_weights_unmasked = F.softmax(scores_unmasked, dim=-1)\n",
        "attn_weights_masked_padding = F.softmax(scores_masked_padding, dim=-1)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.heatmap(scores_unmasked.squeeze().detach().numpy(), annot=True, cmap=\"coolwarm\", fmt=\".1f\", ax=axes[0], cbar=False)\n",
        "axes[0].set_title(\"Raw Scores (No Mask)\")\n",
        "axes[0].set_xlabel(\"Key Position\")\n",
        "axes[0].set_ylabel(\"Query Position\")\n",
        "\n",
        "sns.heatmap(padding_mask.squeeze().detach().numpy(), annot=True, cmap=\"gray\", fmt=\".0f\", ax=axes[1], cbar=False)\n",
        "axes[1].set_title(\"Padding Mask (0 = Masked)\")\n",
        "axes[1].set_xlabel(\"Key Position\")\n",
        "\n",
        "\n",
        "sns.heatmap(attn_weights_masked_padding.squeeze().detach().numpy(), annot=True, cmap=\"viridis\", fmt=\".2f\", ax=axes[2])\n",
        "axes[2].set_title(\"Attention Weights (With Padding Mask)\")\n",
        "axes[2].set_xlabel(\"Key Position\")\n",
        "\n",
        "plt.suptitle(\"Effect of Padding Mask on Attention Weights\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "print(\"Observation: Notice how the last two columns in the final attention weights are near zero due to the padding mask.\")"
      ],
      "metadata": {
        "id": "C1DpLyxGD10Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Look-Ahead Mask Example ---\n",
        "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
        "\n",
        "# Use the same unmasked scores from before\n",
        "scores_masked_lookahead = scores_unmasked.masked_fill(look_ahead_mask == 0, -1e9)\n",
        "\n",
        "# Calculate attention weights (softmax)\n",
        "attn_weights_masked_lookahead = F.softmax(scores_masked_lookahead, dim=-1)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.heatmap(scores_unmasked.squeeze().detach().numpy(), annot=True, cmap=\"coolwarm\", fmt=\".1f\", ax=axes[0], cbar=False)\n",
        "axes[0].set_title(\"Raw Scores (No Mask)\")\n",
        "axes[0].set_xlabel(\"Key Position\")\n",
        "axes[0].set_ylabel(\"Query Position\")\n",
        "\n",
        "sns.heatmap(look_ahead_mask.squeeze().int().detach().numpy(), annot=True, cmap=\"gray\", fmt=\".0f\", ax=axes[1], cbar=False)\n",
        "axes[1].set_title(\"Look-Ahead Mask (0 = Masked)\")\n",
        "axes[1].set_xlabel(\"Key Position\")\n",
        "\n",
        "sns.heatmap(attn_weights_masked_lookahead.squeeze().detach().numpy(), annot=True, cmap=\"viridis\", fmt=\".2f\", ax=axes[2])\n",
        "axes[2].set_title(\"Attention Weights (With Look-Ahead Mask)\")\n",
        "axes[2].set_xlabel(\"Key Position\")\n",
        "\n",
        "\n",
        "plt.suptitle(\"Effect of Look-Ahead Mask on Attention Weights\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "print(\"Observation: Notice how the upper triangle (excluding the diagonal) in the final attention weights is near zero due to the look-ahead mask. Each query position can only attend to itself and previous key positions.\")"
      ],
      "metadata": {
        "id": "amOzXFcDKmM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Final Linear Layer and Softmax\n",
        "\n",
        "After the final Decoder layer produces its output representations (shape: `batch_size, target_seq_len, d_model`), we need to convert these back into probabilities over the vocabulary for each position.\n",
        "\n",
        "1.  **Linear Layer:** A final linear layer (without bias is common, but depends on implementation) projects the `d_model`-dimensional vector for each position into a `vocab_size`-dimensional vector (logits).\n",
        "    *   Input: `(batch_size, target_seq_len, d_model)`\n",
        "    *   Weight Matrix: `(d_model, vocab_size)`\n",
        "    *   Output (Logits): `(batch_size, target_seq_len, vocab_size)`\n",
        "\n",
        "2.  **Softmax:** The logits are converted into probabilities using the softmax function, applied independently at each position along the `vocab_size` dimension.\n",
        "  * $P(y_i | y_{<i}, x) = \\text{softmax}(\\text{Linear}(\\text{decoder_output}_i))$\n",
        "  *   The output represents the probability distribution over the entire vocabulary for the next token at each position.\n",
        "\n",
        "During inference, we typically select the token with the highest probability (greedy decoding) or use more advanced sampling strategies like beam search, top-k sampling, or nucleus sampling."
      ],
      "metadata": {
        "id": "br6F0aguDKtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 'decoder_output' is the output from the last Decoder layer\n",
        "# Example using the T5 output logits\n",
        "final_decoder_output = outputs_t5.logits.cpu() # Logits already computed by T5 model\n",
        "# Shape: (batch_size, target_seq_len, vocab_size)\n",
        "\n",
        "print(f\"--- Final Output ---\")\n",
        "print(f\"Decoder Output (Logits) shape: {final_decoder_output.shape}\")\n",
        "\n",
        "# Apply Softmax to get probabilities\n",
        "probabilities = F.softmax(final_decoder_output, dim=-1)\n",
        "print(f\"Probabilities shape: {probabilities.shape}\")\n",
        "\n",
        "# Find the token with the highest probability at each position\n",
        "predicted_token_ids = torch.argmax(probabilities, dim=-1) # Shape: (batch_size, target_seq_len)\n",
        "print(f\"Predicted Token IDs shape: {predicted_token_ids.shape}\")\n",
        "\n",
        "# Decode the predicted IDs\n",
        "predicted_tokens = tokenizer_t5.convert_ids_to_tokens(predicted_token_ids[0])\n",
        "predicted_sentence = tokenizer_t5.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nTarget Tokens (Input to Decoder): {decoder_tokens}\")\n",
        "print(f\"Predicted Tokens (Greedy):        {predicted_tokens}\")\n",
        "print(f\"Predicted Sentence (Greedy):      '{predicted_sentence}'\")\n",
        "# Note: The predicted sentence might not exactly match the target_text used as input.\n",
        "# This is because we fed the *entire* target sequence at once (teacher forcing style).\n",
        "# True autoregressive generation builds the sequence step-by-step."
      ],
      "metadata": {
        "id": "c5Q8p8-gy7_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion & Further Exploration\n",
        "\n",
        "We have journeyed through the core components of the Transformer architecture:\n",
        "\n",
        "1.  **Tokenization:** Breaking text into manageable pieces (subwords) and mapping them to IDs.\n",
        "2.  **Embeddings & Positional Encoding:** Converting IDs to vectors and injecting sequence order information.\n",
        "3.  **Encoder:** Processing the input sequence using Self-Attention and Feed-Forward networks to build contextualized representations.\n",
        "4.  **Decoder:** Generating the output sequence using Masked Self-Attention (autoregressive property), Cross-Attention (linking to input), and Feed-Forward networks.\n",
        "5.  **Final Projection:** Converting final representations to vocabulary probabilities.\n",
        "\n",
        "**Key Takeaways & Practical Tips Recap:**\n",
        "\n",
        "*   **Attention is Powerful:** It allows modeling long-range dependencies effectively.\n",
        "*   **Multi-Head Attention:** Captures diverse relationships simultaneously.\n",
        "*   **Masking is Crucial:** For handling padding and ensuring autoregressive behavior in the decoder.\n",
        "*   **Residuals & LayerNorm:** Essential for training deep networks.\n",
        "*   **Pre-trained Models:** Leverage models trained on massive datasets (like BERT, T5, GPT) via libraries like Hugging Face `transformers`. Always match the tokenizer to the model.\n",
        "*   **Visualization Helps:** Understanding attention patterns provides insight into model behavior.\n",
        "\n",
        "**Where to go from here?**\n",
        "\n",
        "*   **Explore Different Architectures:** Dive into BERT (Encoder-only), GPT (Decoder-only) specifics.\n",
        "*   **Fine-tuning:** Learn how to adapt pre-trained models to specific downstream tasks (classification, QA, summarization, etc.).\n",
        "*   **Generation Strategies:** Investigate beam search, top-k, nucleus sampling for better text generation.\n",
        "*   **Efficiency:** Look into techniques like knowledge distillation, quantization, and efficient attention variants (e.g., Linformer, Performer).\n",
        "*   **Implement a Full Transformer:** Try building and maybe even training a small Transformer from scratch on a toy task.\n",
        "*   **Read the Paper:** Go back to \"Attention Is All You Need\" now with a deeper understanding."
      ],
      "metadata": {
        "id": "rUbWzjpkDhmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Vision Transformers (ViT)"
      ],
      "metadata": {
        "id": "wljVHTvfz6f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Understand how the main ideas of Tranformers for NLP (Module 1) can be transferred to computer vision. We will explore how tokenization, embeddings, positional encodings, attention are realized for images.\n",
        "\n",
        "**Approach**: We will leverage the Hugging Face transformers library to finetune and inspect the key components of Vision Transformer for image classification."
      ],
      "metadata": {
        "id": "wf2iYoRc0C7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "IciTDSNW0HnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y08Z-WbE0AMT",
        "outputId": "5ce2fdb1-a9bc-46cd-d3f5-16b5f8ac29bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "import cv2\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "0Jd-rTue0N6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_distance_matrix(patch_size, num_patches, length):\n",
        "    distance_matrix = np.zeros((num_patches, num_patches))\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            if i == j:  # zero distance\n",
        "                continue\n",
        "\n",
        "            xi, yi = (int(i / length)), (i % length)\n",
        "            xj, yj = (int(j / length)), (j % length)\n",
        "            distance_matrix[i, j] = patch_size * np.linalg.norm([xi - xj, yi - yj])\n",
        "\n",
        "    return distance_matrix"
      ],
      "metadata": {
        "id": "WB7q33ku0OGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "The Vision Transformer (ViT) architecture was proposed in the paper [\"An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale\"](https://arxiv.org/pdf/2010.11929) by Dosovitskiy et al.\n",
        "\n",
        "Inspired by success of Transformers in NLP, the authors proposed to apply Transformer architecture to images, with the fewest possible modifications.\n",
        "\n",
        "When pre-trained on large datasets and transferred to tasks with fewer datapoints, ViT achieves performance comparable to state-of-the-art convolutional networks while requiring fewer computational resources to train.\n",
        "\n",
        "**High-level architecture**\n",
        "\n",
        "![](https://yastatic.net/s3/education-portal/media/Vi_T_c7f1bfbedc_e047c467f5.webp)\n",
        "\n",
        "The main components of ViT:\n",
        "1. **Linear projection**: an image is converted into a set of vectors as Transformer input.\n",
        "2. **Transformer Encoder**: encoder similar to encoder for NLP.\n",
        "3. **MLP Head**: fully-connected classification head."
      ],
      "metadata": {
        "id": "oLSmncwt0W3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ViT architecture\n",
        "\n",
        "### 2.1 Linear Projection\n",
        "\n",
        "To handle $2D$ images, we need to convert them to a sequence of embeddings.\n",
        "\n",
        "Let $H, W$ - spatial height and width, $C$ - number of channels.\n",
        "\n",
        "1. Split an image $x\\in\\mathbb{R}^{H\\times W \\times C}$ into patches of shape $P\\times P \\times C$.\n",
        "\n",
        "2. Reshape the patches of $P \\times P \\times C$ into vectors of shape $P \\times P \\times C$.\n",
        "\n",
        "Thus, we obtain $x_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$, where $N = HW / P^2$ - number of patches (also serves as the effective input sequence length for the Transformer). Image patches are treated the same way as tokens (words) in NLP.\n",
        "\n",
        "3. Obtain patch embeddings via flattenning and mapping to $D$-dimensional vectors.\n",
        "\n",
        "$$z_0 = [x_{class}; x_p^1\\pmb{E}; x_p^2\\pmb{E}; \\dots x_p^N\\pmb{E}] + \\pmb{E}_{pos}, \\quad \\pmb{E}\\in \\mathbb{R}^{(P^2\\cdot C) \\times D}, \\quad \\pmb{E}_{pos} \\in \\mathbb{R}^{(N + 1) \\times D}$$\n",
        "\n",
        "Note: similar to BERT [CLS] token, a learnable embedding $z_0^0 = x_{class}$ is added to the patch embeddings.\n",
        "\n",
        "4. Similar to NLP, position embeddings are added to patch embeddings to retain positional information. Position embeddings are learnable $1D$ parameters.\n",
        "\n",
        "### 2.2 Transformer Encoder\n",
        "\n",
        "5. The resulting sequence $z_0$ of embedding vectors is input to Transformer model with alternating Multihead Self-Attention layers (MSA) and MLP blocks. LayerNorm (LN) is applied before each block, and residual connections after each block.\n",
        "\n",
        "$$z^\\prime_l = \\text{MSA}(\\text{LN}(z_{l-1})) + z_{l-1}, \\quad l=1 \\dots L$$\n",
        "\n",
        "$$z_l = \\text{MLP}(\\text{LN}(z^\\prime_l)) + z^\\prime_l, \\quad l=1 \\dots L$$\n",
        "\n",
        "### 2.3 MLP Head\n",
        "\n",
        "6. Classification token's state at the output of the Transformer encoder $z_L^0$ serves as the image representation $y$.\n",
        "\n",
        "$$y = \\text{LN}(z_L^0)$$\n",
        "\n",
        "The classification head is implemented by a MLP. The model on image classification is trained in a supervised way.\n"
      ],
      "metadata": {
        "id": "lPcBVNdy0ilQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Practice\n",
        "\n",
        "Now, let us perform pre-trained ViT finetuning and inspect its key components.\n",
        "\n",
        "### 3.1 Dataset Preprocessing\n",
        "\n",
        "First, let us load the dataset. While the original dataset contains $101$ classes, we will use only $20$ first classes for faster convergence."
      ],
      "metadata": {
        "id": "fbxLUm2c0lvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "dataset_path = \"ethz/food101\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset_raw = load_dataset(dataset_path)\n",
        "N_LABELS = 20 # set the desired number of classes\n",
        "labels_full = dataset_raw['train'].features['label'].names\n",
        "labels = labels_full[:N_LABELS]\n",
        "idx2label = {i: l for i, l in enumerate(labels)}\n",
        "\n",
        "# Filter dataset: choose examples with desired classes\n",
        "dataset_raw = dataset_raw.filter(lambda example: example['label'] in idx2label and example['image'].mode == \"RGB\")\n",
        "dataset_raw"
      ],
      "metadata": {
        "id": "TKWfi7QS0bfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize some images from the original dataset."
      ],
      "metadata": {
        "id": "_SmbSDJX06aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a batch of random images\n",
        "np.random.seed(0)\n",
        "random_idx = np.random.choice(len(dataset_raw['validation']), size=10, replace=False)\n",
        "batch = dataset_raw['validation'][random_idx]\n",
        "\n",
        "# Plot examples\n",
        "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(25, 10), tight_layout=True)\n",
        "ax = ax.flatten()\n",
        "for i, (img, label) in enumerate(zip(batch['image'], batch['label'])):\n",
        "  ax[i].imshow(img)\n",
        "  ax[i].set_title(\"Class: {}\\n Size: {}x{}\".format(idx2label[label], img.size[0], img.size[1]))"
      ],
      "metadata": {
        "id": "9MYGCCFe0zZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finetune ViT model, we will apply the corresponding image preprocessing as dataset transform."
      ],
      "metadata": {
        "id": "APCJToWk1C57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the pre-trained model path\n",
        "model_name = 'google/vit-base-patch16-224'\n",
        "# Load the ViT image Processor\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "jx_oeZme5Zez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define transform function using the ViTImageProcessor and apply to the dataset\n",
        "\n",
        "**Task**:\n",
        "1. Apply processor from prev cell for an each image in the batch."
      ],
      "metadata": {
        "id": "YDV0Q7Um5aa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(batch):\n",
        "    # Apply processor to images from the batch\n",
        "    proc_batch = <YOUR_CODE>\n",
        "    # Copy the image labels\n",
        "    proc_batch['labels'] = batch['label']\n",
        "    return proc_batch\n",
        "\n",
        "# Apply the defined transform to the dataset\n",
        "dataset = dataset_raw.with_transform(transform)"
      ],
      "metadata": {
        "id": "FS9B1TNw0zgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the images after transform."
      ],
      "metadata": {
        "id": "YZCori8g1Izn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the same batch from the transformed dataset\n",
        "subset = dataset['validation'][random_idx]\n",
        "mean = torch.tensor(processor.image_mean)\n",
        "std = torch.tensor(processor.image_std)\n",
        "\n",
        "# Plot exmaples\n",
        "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(25, 10), tight_layout=True)\n",
        "ax = ax.flatten()\n",
        "# After transform, 'image' is replaced with 'pixel_values', 'label' - with 'labels'\n",
        "for i, (img, label) in enumerate(zip(subset['pixel_values'], subset['labels'])):\n",
        "  proc_img = img.permute((1, 2, 0)) * std + mean\n",
        "  ax[i].imshow(proc_img)\n",
        "  ax[i].set_title(\"Class: {}\\n Size: {}x{}\".format(idx2label[label], proc_img.shape[0], proc_img.shape[1]))"
      ],
      "metadata": {
        "id": "wyQzgGju0zjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Model Training\n",
        "\n",
        "To train the model, we need some utils.\n",
        "\n",
        "Define collate function to form batches"
      ],
      "metadata": {
        "id": "wYd4ZXJW1M1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "wvovmc3R0zmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and define accuracy metric"
      ],
      "metadata": {
        "id": "VPN8xB8g1WtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_input):\n",
        "  # Load the metric\n",
        "  accuracy_metric = evaluate.load(\"accuracy\")\n",
        "  logits, labels = eval_input\n",
        "  # Compute predictions\n",
        "  preds = np.argmax(logits, axis=1)\n",
        "  return accuracy_metric.compute(predictions=preds, references=labels)"
      ],
      "metadata": {
        "id": "yJPjhnlZ1Tzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained ViT\n",
        "\n",
        "**Task**:\n",
        "1. Fill in basic arguments in functions\n",
        "2. Use 1 epoch with 0.0001 LR and batch_size of 64."
      ],
      "metadata": {
        "id": "OeCOgync1Y9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify model name, number of clasees\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(idx2label),\n",
        "    # We change the number of classes\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sdzlBBvt1T2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup training arguments and trainer"
      ],
      "metadata": {
        "id": "5X88B1ud1h6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose batch sizes to fit GPU memory\n",
        "TRAIN_BATCH_SIZE = <YOUR_CODE>\n",
        "EVAL_BATCH_SIZE =  <YOUR_CODE>\n",
        "# Hint: We said 1, because 1 epoch is enough, training is not fast\n",
        "NUM_EPOCHS = <YOUR_CODE>\n",
        "# Set appropriate learning rate\n",
        "LR = <YOUR_CODE>"
      ],
      "metadata": {
        "id": "pL1O__bs1T5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "  # Set desired value\n",
        "  output_dir=None,\n",
        "  per_device_train_batch_size=<YOUR_CODE>,\n",
        "  per_device_eval_batch_size=<YOUR_CODE>,\n",
        "  num_train_epochs=<YOUR_CODE>,\n",
        "  learning_rate=<YOUR_CODE>,\n",
        "  # Set logging frequency\n",
        "  logging_steps=50,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  fp16=True,\n",
        "  remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "TCZlfPJK1T8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    # Model\n",
        "    model=<YOUR_CODE>,\n",
        "    # Training args\n",
        "    args=<YOUR_CODE>,\n",
        "    # Collate function\n",
        "    data_collator=collate_fn,\n",
        "    # Metric function\n",
        "    compute_metrics=compute_metrics,\n",
        "    # Train dataset\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    # Validation dataset\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    # ViT ImageProcessor\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdGQ1UXE1T_a",
        "outputId": "dbf5a49c-1dcc-4d8c-81a0-3825e9a48301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-620ba783073c>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "JqyJqoMS1pYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SWrh6wD10zpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate finetuned model"
      ],
      "metadata": {
        "id": "Rn0FvlwQ1uxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "q99gn2Il1t4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Inspect Linear Projection\n",
        "\n",
        "Choose an example image, get model prediction\n",
        "\n",
        "**Task:**\n",
        "* How to we get prediction from logits? Do you remember? Realize it."
      ],
      "metadata": {
        "id": "jqmDG1SC100z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image from the validation dataset\n",
        "img = dataset['validation'][0]['pixel_values']\n",
        "# True label\n",
        "true_label = dataset['validation'][0]['labels']\n",
        "# Obtain logits\n",
        "logits = model(img.unsqueeze(0).to('cuda')).logits.detach().cpu()\n",
        "# Get predictions\n",
        "pred_label = <YOUR_CODE>\n",
        "# Inverse process image\n",
        "img = img.permute((1, 2, 0)) * std + mean"
      ],
      "metadata": {
        "id": "O5cXi1SH1t7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the image"
      ],
      "metadata": {
        "id": "0xUhdTql16nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img)\n",
        "plt.title('True label: {},\\n Predicted label: {}'.format(idx2label[true_label], idx2label[pred_label]))"
      ],
      "metadata": {
        "id": "xuYPC_tH1t_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us visualize image patches. First, get patch size and number of patches"
      ],
      "metadata": {
        "id": "q4OdhDEV2AWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get patch size from model config\n",
        "PATCH_SIZE = model.config.patch_size\n",
        "# Get spatial sizes from processor\n",
        "H, W = processor.size['height'], processor.size['width']\n",
        "# Compute number of patches\n",
        "NUM_PATCHES = (H * W) // (PATCH_SIZE * PATCH_SIZE)\n",
        "PATCH_SIZE, NUM_PATCHES"
      ],
      "metadata": {
        "id": "mZO755_w1uB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example patches\n",
        "patches = img.unfold(0, PATCH_SIZE, PATCH_SIZE).unfold(1, PATCH_SIZE, PATCH_SIZE).permute((0, 1, 3, 4, 2))\n",
        "patches.shape"
      ],
      "metadata": {
        "id": "iVb7lhA51uE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H_NUM_PATCHES, W_NUM_PATCHES = patches.shape[:2]\n",
        "H_NUM_PATCHES, W_NUM_PATCHES"
      ],
      "metadata": {
        "id": "q8J-JNxK1uHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize patches\n",
        "fig, ax = plt.subplots(H_NUM_PATCHES, W_NUM_PATCHES)\n",
        "for i in range(H_NUM_PATCHES):\n",
        "    for j in range(W_NUM_PATCHES):\n",
        "        p = patches[i, j]\n",
        "        ax[i][j].imshow(p)\n",
        "        ax[i][j].axis('off')"
      ],
      "metadata": {
        "id": "2HBClCUw2J-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ViT, each patch is projected to $D$-dimensional vector with the help of ViT. Define a convolution layer, that would do this linear projection.\n",
        "\n",
        "**Task:**\n",
        "* Fill in the blanks.\n",
        "\n",
        "[Hint: use model.config to derive input and output channels]"
      ],
      "metadata": {
        "id": "4-M0pR5d2Pe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input chnnels\n",
        "in_channels = <YOUR_CODE>\n",
        "# Output channels [Hint: this is D]\n",
        "hidden_size = <YOUR_CODE>\n",
        "# Define convoluton parameters\n",
        "kernel_size = <YOUR_CODE>\n",
        "stride = <YOUR_CODE>\n",
        "padding = <YOUR_CODE>\n",
        "# Define projection\n",
        "projection_layer = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=hidden_size,\n",
        "                             kernel_size=kernel_size,\n",
        "                             stride=stride,\n",
        "                             padding=padding)"
      ],
      "metadata": {
        "id": "2CRq_lZu2KA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare your result with model projection layer.\n",
        "\n",
        "Visualize learned projection convolution weights."
      ],
      "metadata": {
        "id": "3Yl9gfhc2Spf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Projection convolution weight\n",
        "weight = model.vit.embeddings.patch_embeddings.projection.weight\n",
        "weight = weight.detach().cpu().permute((2, 3, 1, 0)).numpy()\n",
        "print ('Weight shape: ', weight.shape)\n",
        "# Scale values for visualization\n",
        "scaled_weight = MinMaxScaler().fit_transform(weight.reshape(-1, hidden_size))\n",
        "scaled_weight = scaled_weight.reshape(PATCH_SIZE, PATCH_SIZE, in_channels, -1)\n",
        "# Visualize first 25 kernels\n",
        "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(5*1, 5*1), tight_layout=True)\n",
        "axes = axes.flatten()\n",
        "for i in range(25):\n",
        "    axes[i].imshow(scaled_weight[:, :, :, i])\n",
        "    axes[i].axis(\"off\")"
      ],
      "metadata": {
        "id": "_SoIWK7_2KDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learned convolution filters look like reasonable low-dimensional patterns of patches.\n",
        "\n",
        "### 3.4 Inspect positional embeddings\n",
        "\n",
        "Explore similarity between learned positional embeddings.\n",
        "\n",
        "First, extract positional encodings. Do not account for classification token.\n",
        "\n",
        "**Task:**\n",
        "* Realize basic cos sim function."
      ],
      "metadata": {
        "id": "-TsK8SKz2ba8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not forger to do detach\n",
        "pos_embeddings = model.vit.embeddings.position_embeddings.squeeze().detach().cpu()[1:]\n",
        "pos_embeddings.shape"
      ],
      "metadata": {
        "id": "RKmJU8HP2KGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize embeddings\n",
        "norm_pos_embeddings = (pos_embeddings / pos_embeddings.norm(dim=1, keepdim=True))\n",
        "# Compute cosine similarity\n",
        "cos_sim = <YOUR_CODE>"
      ],
      "metadata": {
        "id": "ZPpZCoOj2lTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visalize cosine similairty"
      ],
      "metadata": {
        "id": "Fr0IClbs2sMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = plt.imshow(cos_sim)\n",
        "plt.colorbar(pos, fraction=0.05)"
      ],
      "metadata": {
        "id": "Qzk0cy8X2lWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a clear diagonal pattern indicating that position is the most similar to itself. Also, there are repeated diagonal patterns resembling positional encodings in NLP.\n",
        "\n",
        "### 3.5 Attention Distance"
      ],
      "metadata": {
        "id": "7rQ90zJP2uxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = dataset['validation'][0]['pixel_values']\n",
        "outputs = model(img.unsqueeze(0).to('cuda'), output_attentions=True)\n",
        "attn_tensor = torch.stack([x.detach().cpu() for x in outputs['attentions']]).squeeze().numpy()\n",
        "# Attention has shape (batch_size x num_layers x num_heads x (num_pathes + 1) x (num_patches + 1))\n",
        "print(attn_tensor.shape)"
      ],
      "metadata": {
        "id": "fPSGfj132lZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute distances between patch coordinates\n",
        "distance_matrix = compute_distance_matrix(PATCH_SIZE, NUM_PATCHES, H_NUM_PATCHES)[None, None, :]\n",
        "# Do not account for classification token\n",
        "mean_distances = attn_tensor[:, :, 1:, 1:] * distance_matrix\n",
        "# Sum over attention per token\n",
        "mean_distances = np.sum(mean_distances, axis=-1)\n",
        "# Mean over tokens\n",
        "mean_distances = np.mean(mean_distances, axis=-1)"
      ],
      "metadata": {
        "id": "_TnNSTbi2lcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = mean_distances.shape[-1]\n",
        "for idx in range(len(mean_distances)):\n",
        "    mean_dist = mean_distances[idx]\n",
        "    x = [idx] * num_heads\n",
        "    plt.scatter(x=x, y=mean_dist, label=f\"transformer_block_{idx}\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"Attention Head\", fontsize=14)\n",
        "plt.ylabel(\"Attention Distance\", fontsize=14)\n",
        "plt.title(model_name, fontsize=14)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lQKebK-u2lfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different attention heads may have different attention distances using both local and global information of the image. When depth increases, transformer layers focus on global information.\n",
        "\n",
        "### 3.6 Attention Heatmaps\n",
        "\n",
        "Finally, visualize the attention maps.\n",
        "\n",
        "First, use the same validation image with inverse transforms as an example."
      ],
      "metadata": {
        "id": "Lv9icH_V2-NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = dataset['validation'][0]['pixel_values']\n",
        "img = img.permute((1, 2, 0)) * std + mean\n",
        "img.shape"
      ],
      "metadata": {
        "id": "kvKpCKPr3IBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we will look at final layer and classification token."
      ],
      "metadata": {
        "id": "BSePcB4I3Kpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_attn_cls = attn_tensor[-1, :, 0, 1:].reshape((-1, H_NUM_PATCHES, W_NUM_PATCHES))\n",
        "last_attn_cls.shape"
      ],
      "metadata": {
        "id": "HAvCQBGZ3InH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the size of attention map is smaller that image size, use resize to make it similar to image."
      ],
      "metadata": {
        "id": "wCYc50H43NRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_attn_cls = np.stack([cv2.resize(a, dsize=(H, W)) for a in last_attn_cls])\n",
        "last_attn_cls.shape"
      ],
      "metadata": {
        "id": "k2EhNJHI3IqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot image together with attention maps for each head."
      ],
      "metadata": {
        "id": "O9AY68AF3Ru9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(6*2, 2*2), tight_layout=True)\n",
        "axes = axes.flatten()\n",
        "for i, ax in enumerate(axes):\n",
        "  ax.imshow(img)\n",
        "  ax.imshow(last_attn_cls[i], alpha=0.6)\n",
        "  ax.title.set_text(f\"Attention head: {i}\")\n",
        "  ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "rKO2S7Km3Isz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention maps are computed between the tokens of the same image. Thus, attention scores highlight which parts of images are important. This example illustrates the purpose of attention mechanism."
      ],
      "metadata": {
        "id": "nS-G7qNp3XsO"
      }
    }
  ]
}