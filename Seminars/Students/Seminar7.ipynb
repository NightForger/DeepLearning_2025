{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# News Stream Deduplication with Metric Learning (Contrastive Loss)\n",
        "\n",
        "Our goal is to train a model that can embed news articles into a vector space such that similar articles are close together, and dissimilar articles are far apart. We will then use these embeddings to identify duplicates.\n",
        "\n",
        "In this notebook, you will:\n",
        "1.  Set up the environment and download the required datasets from Hugging Face.\n",
        "2.  Load and explore the news data.\n",
        "3.  Prepare the data for training (generating positive and negative pairs).\n",
        "4.  Set up an appropriate embedding model and the Contrastive Loss function.\n",
        "5.  Train the model on the prepared data (several times).\n",
        "6.  Use the trained model to find duplicates in a test set.\n",
        "\n",
        "** It is very simplified version of real code:**\n",
        "1. We use very crude mining process, but simple and efficient to understand what is happening.\n",
        "2. We use only 2 layers of data instead of 3.\n",
        "3. We train only on subsample of data, because real training takes 3 hours."
      ],
      "metadata": {
        "id": "ce_iCO8diiji"
      },
      "id": "ce_iCO8diiji"
    },
    {
      "cell_type": "markdown",
      "id": "3a875eb2-b14a-4c08-bafe-b0a683b1f424",
      "metadata": {
        "id": "3a875eb2-b14a-4c08-bafe-b0a683b1f424"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d005807-6019-42d7-85b8-65bfb1c1de7f",
      "metadata": {
        "id": "0d005807-6019-42d7-85b8-65bfb1c1de7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537c1151-9e38-41da-d6cb-0d73d0d63c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets >> None\n",
        "#%pip install sentence-transformers[train]==3.0.1 >> None\n",
        "%pip install -U sentence-transformers transformers accelerate\n",
        "\n",
        "from random import randint, choice\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Union, List\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import requests\n",
        "import random\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"\n",
        "    Sets the seed for reproducibility across multiple libraries (random, numpy, torch, tensorflow).\n",
        "\n",
        "    Args:\n",
        "        seed_value (int): The integer value to use as the seed. Default is 42.\n",
        "    \"\"\"\n",
        "    print(f\"Setting seed to {seed_value}\")\n",
        "\n",
        "    # 1. Set `PYTHONHASHSEED` environment variable (optional but good practice)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "    # 2. Set the `python` built-in random number generator\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # 3. Set the `numpy` random number generator\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # 4. Set the `pytorch` random number generator (if torch is installed)\n",
        "    if 'torch' in globals() and torch.__version__:\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed_value) # Seed all GPUs\n",
        "            torch.manual_seed(seed_value)\n",
        "        except Exception as e:\n",
        "             pass\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJH3JSX8BLBk",
        "outputId": "f59a7903-0cfc-4666-9487-0ea025413910"
      },
      "id": "fJH3JSX8BLBk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting seed to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_hf_datasets(dataset_name=\"AAAAAA2121/file_host\", subfolder=\"main\", files=None, save_dir=\"./data\"):\n",
        "    \"\"\"\n",
        "    Downloads specified files from a Hugging Face dataset repository.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): The namespace/dataset_name on Hugging Face.\n",
        "        subfolder (str): The branch or subfolder within the dataset (usually 'main').\n",
        "        files (list): A list of filenames to download. If None, attempts to download\n",
        "                      the default files specified in the function.\n",
        "        save_dir (str): The local directory to save the files to.\n",
        "    \"\"\"\n",
        "    if files is None:\n",
        "        files = [\n",
        "            \"4traingpt.csv\",\n",
        "            \"4trainnyan.csv\",\n",
        "            \"gpt100k.csv\",\n",
        "            \"nyan.csv\"\n",
        "        ]\n",
        "\n",
        "    base_url = f\"https://huggingface.co/datasets/{dataset_name}/resolve/{subfolder}/\"\n",
        "\n",
        "    # Create the save directory if it doesn't exist\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        print(f\"Created directory: {save_dir}\")\n",
        "\n",
        "    print(f\"Downloading files from {base_url}...\")\n",
        "\n",
        "    for file_name in files:\n",
        "        file_url = base_url + file_name\n",
        "        save_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "        # Check if file already exists\n",
        "        if os.path.exists(save_path):\n",
        "            print(f\"File already exists: {file_name}. Skipping download.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            # Use stream=True for large files and process in chunks\n",
        "            with requests.get(file_url, stream=True) as r:\n",
        "                r.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(f\"Successfully downloaded {file_name} to {save_path}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {file_name}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while downloading {file_name}: {e}\")\n",
        "\n",
        "    print(\"Download process finished.\")"
      ],
      "metadata": {
        "id": "s0R4Defx2R9n"
      },
      "id": "s0R4Defx2R9n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7f8fbeac-7596-4d52-ab22-ea6a75597119",
      "metadata": {
        "id": "7f8fbeac-7596-4d52-ab22-ea6a75597119"
      },
      "source": [
        "# Data\n",
        "In this section we will download, prepare and mine neg samples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_hf_datasets()"
      ],
      "metadata": {
        "id": "3kb4zRbGFZiN"
      },
      "id": "3kb4zRbGFZiN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Negative Samples\n",
        "\n",
        "For training models with **Contrastive Loss**, we need pairs of data points:\n",
        "* **Positive Pairs:** Items that are considered similar or duplicates.\n",
        "* **Negative Pairs:** Items that are considered dissimilar or non-duplicates.\n",
        "\n",
        "While creating positive pairs is usually straightforward (e.g., based on a 'true' duplicate ID, if available, or manually defined rules), generating effective *negative* pairs is crucial for successful training and often requires a specific strategy. Simply pairing random articles might result in negatives that are too easy for the model to distinguish. Your `generate_neg_sample` function implements a custom strategy for **structured negative sampling**.\n",
        "\n",
        "**Function Purpose:**\n",
        "This function iterates through each item in the input DataFrame (`df`) and attempts to find a *different* item to form a negative pair with it. It aims to create negative examples that are not just random, but potentially from related categories or different text types, making the learning task more robust.\n",
        "\n",
        "**How it Works (Negative Sampling Strategy):**\n",
        "\n",
        "For each item processed from your input DataFrame (let's call the text from this item `post_1`), the function randomly selects a potential source for its corresponding negative sample (`post_2`) based on two criteria extracted from your data:\n",
        "\n",
        "1.  **Category:** Should `post_2` come from the *same* category as `post_1` or a *different* category?\n",
        "2.  **Text Type:** Assuming your DataFrame has columns for original and potentially generated text (like 'gpt'), should `post_2` be of the *same* text type (original or GPT) as `post_1`, or the *other* text type?\n",
        "\n",
        "This combination defines 4 different \"modes\" of sampling, which are chosen randomly (`randint(1,4)`) for each negative pair generated:\n",
        "\n",
        "* **Mode 1:** Sample `post_2` from the **same category** as `post_1`, using the **original** text from that item.\n",
        "* **Mode 2:** Sample `post_2` from the **same category** as `post_1`, using the **GPT-generated** text from that item.\n",
        "* **Mode 3:** Sample `post_2` from a **different category** than `post_1`, using the **original** text from that item.\n",
        "* **Mode 4:** Sample `post_2` from a **different category** than `post_1`, using the **GPT-generated** text from that item.\n",
        "\n",
        "**Finding a Unique Sample:**\n",
        "\n",
        "After selecting a mode, the function samples an item according to that mode's criteria. It includes a check (`while post_2 == post_1:`) to ensure that the selected `post_2` is *not identical* to the original `post_1`. It will keep re-sampling from the chosen mode's pool until it finds a distinct text.\n",
        "\n",
        "**Handling Difficult Cases:**\n",
        "\n",
        "* If it fails to find a distinct sample within 3 attempts using the initially chosen mode, the sampling `mode` is re-randomized to try a different strategy.\n",
        "* If it still cannot find a distinct sample after 10 attempts in total (which could happen if a category is extremely small or contains only identical texts), it gives up and assigns `np.nan` as `post_2` for that specific pair, then moves to the next item.\n",
        "\n",
        "**Output:**\n",
        "The function iterates through the entire input DataFrame (`df`) and for each row, generates one negative pair based on the logic described above. It returns a new pandas DataFrame containing two columns, `post_1` and `post_2`, where each row represents a generated negative pair designed to be dissimilar.\n",
        "\n",
        "This strategic approach to negative sampling, varying the source category and text type, helps create more diverse and potentially more challenging negative examples for the model to learn from. This is a form of **Negative Mining**, aiming to select negatives that are not trivially different, thereby improving the model's ability to make fine-grained distinctions."
      ],
      "metadata": {
        "id": "xgV7uDPl9rxf"
      },
      "id": "xgV7uDPl9rxf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task:\n",
        "1. Try to write your own negative mining function. For example, you can use simple mining strategy - random picking from different class.\n",
        "2. Check negative examples in the result.\n"
      ],
      "metadata": {
        "id": "MjtmFk0cLYRl"
      },
      "id": "MjtmFk0cLYRl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import choice, randint\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_neg_sample(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates negative pairs for Contrastive Loss training by sampling\n",
        "    a different article for each input article based on a random strategy.\n",
        "\n",
        "    The sampling strategy considers the category of the original article\n",
        "    and the type of text (original or generated) for both articles.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing articles.\n",
        "                           Assumes it has a 'category' column and\n",
        "                           at least two text columns (assumed to be the first two).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with 'post_1' and 'post_2' columns\n",
        "                      containing the generated negative pairs.\n",
        "    \"\"\"\n",
        "    neg_pairs = {'post_1': [], 'post_2': []}\n",
        "\n",
        "    # Assuming the first two columns of the DataFrame contain the texts (e.g., original and gpt)\n",
        "    # Adjust text_columns if your text data is in different columns (e.g., ['text_orig', 'text_gpt'])\n",
        "    text_columns = df.columns[:2]\n",
        "    if len(text_columns) < 2:\n",
        "         raise ValueError(\"DataFrame must have at least two text columns.\")\n",
        "    if 'category' not in df.columns:\n",
        "         raise ValueError(\"DataFrame must have a 'category' column.\")\n",
        "\n",
        "\n",
        "    # Define constants for the sampling modes for clarity\n",
        "    MODE_SAME_CAT_ORIG = 1\n",
        "    MODE_SAME_CAT_OTHER_TEXT = 2 # Assumes 'other' text is GPT if orig is text_columns[0]\n",
        "    MODE_DIFF_CAT_ORIG = 3\n",
        "    MODE_DIFF_CAT_OTHER_TEXT = 4\n",
        "\n",
        "    MAX_ATTEMPTS_PER_SAMPLE = 10 # Max attempts to find a unique negative sample\n",
        "    RESAMPLE_MODE_THRESHOLD = 3 # Re-randomize mode after this many failed attempts\n",
        "\n",
        "    print(\"Generating negative samples...\")\n",
        "\n",
        "    for index in tqdm(range(df.shape[0]), desc=\"Generating Negatives\"):\n",
        "        current_row = df.iloc[index]\n",
        "        initial_category = current_row['category']\n",
        "\n",
        "        # Randomly choose which text column from the current row will be 'post_1'\n",
        "        post_1_col_index = randint(0, 1) # 0 or 1\n",
        "        post_1_text = current_row[text_columns[post_1_col_index]]\n",
        "\n",
        "        found_neg_text = None\n",
        "        attempt_count = 0\n",
        "        current_sampling_mode = randint(1, 4) # Start with a random mode\n",
        "\n",
        "        # Loop to find a suitable negative sample (post_2) that is different from post_1\n",
        "        while attempt_count < MAX_ATTEMPTS_PER_SAMPLE:\n",
        "            attempt_count += 1\n",
        "\n",
        "            # --- Step 1: Determine Filtering Criteria based on current_sampling_mode ---\n",
        "            filter_by_same_category = current_sampling_mode in [MODE_SAME_CAT_ORIG, MODE_SAME_CAT_OTHER_TEXT]\n",
        "            category_filter = df['category'] == initial_category if filter_by_same_category else df['category'] != initial_category\n",
        "\n",
        "            # --- Step 2: Determine which Text Column to Sample from ---\n",
        "            # Logic: Modes 1 & 3 sample from the same text type as post_1_col_index.\n",
        "            # Modes 2 & 4 sample from the *other* text type.\n",
        "            if current_sampling_mode in [MODE_SAME_CAT_ORIG, MODE_DIFF_CAT_ORIG]:\n",
        "                 sample_col_index = post_1_col_index # Sample from the same text type column\n",
        "            else: # Modes 2 and 4\n",
        "                 sample_col_index = (post_1_col_index + 1) % 2 # Sample from the other text type column\n",
        "\n",
        "            # --- Step 3: Filter DataFrame and Get List of Candidates ---\n",
        "            candidate_df = df[category_filter]\n",
        "\n",
        "            # Ensure there are candidates to sample from based on the filter\n",
        "            if candidate_df.empty:\n",
        "                 # If no candidates match the filter, try a different mode\n",
        "                 if attempt_count % RESAMPLE_MODE_THRESHOLD == 0 or attempt_count == 1: # Re-randomize mode if filter yields nothing or every 3 attempts\n",
        "                      current_sampling_mode = randint(1, 4)\n",
        "                      # print(f\"   Attempt {attempt_count}: Filtered df empty. Re-randomizing mode to {current_sampling_mode}\") # Optional log\n",
        "                 continue # Skip sampling in this attempt, try again with potentially new mode\n",
        "\n",
        "            candidate_texts = candidate_df[text_columns[sample_col_index]].tolist()\n",
        "\n",
        "            # Ensure the list of potential texts is not empty after selecting column\n",
        "            if not candidate_texts:\n",
        "                 # If list is empty (e.g., column has all NaNs in filtered rows), try different mode\n",
        "                 if attempt_count % RESAMPLE_MODE_THRESHOLD == 0 or attempt_count == 1:\n",
        "                      current_sampling_mode = randint(1, 4)\n",
        "                      # print(f\"   Attempt {attempt_count}: Candidate list empty. Re-randomizing mode to {current_sampling_mode}\") # Optional log\n",
        "                 continue # Skip sampling\n",
        "\n",
        "            # --- Step 4: Randomly Sample and Check if Different ---\n",
        "            sampled_text = choice(candidate_texts)\n",
        "\n",
        "            if sampled_text != post_1_text:\n",
        "                found_neg_text = sampled_text\n",
        "                break # Successfully found a different negative sample\n",
        "\n",
        "            # If sampled_text == post_1_text, the loop continues to the next attempt.\n",
        "            # Check if we should re-randomize the mode for the next attempt.\n",
        "            if attempt_count % RESAMPLE_MODE_THRESHOLD == 0:\n",
        "                 current_sampling_mode = randint(1, 4)\n",
        "                 # print(f\"   Attempt {attempt_count}: Sampled same text. Re-randomizing mode to {current_sampling_mode}\") # Optional log\n",
        "\n",
        "\n",
        "        # --- After the while loop: Append the result ---\n",
        "        # If found_neg_text is still None, it means we failed to find a distinct sample\n",
        "        final_neg_text = found_neg_text if found_neg_text is not None else np.nan\n",
        "\n",
        "        neg_pairs['post_1'].append(post_1_text)\n",
        "        neg_pairs['post_2'].append(final_neg_text)\n",
        "\n",
        "    print(\"Finished generating negative samples.\")\n",
        "    return pd.DataFrame(neg_pairs)\n"
      ],
      "metadata": {
        "id": "tJ5dT4913hf6"
      },
      "id": "tJ5dT4913hf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our GPT labeled data\n",
        "df_gpt = pd.read_csv('./data/gpt100k.csv').rename({'original':'post_1', 'gpt':'post_2'}, axis=1)\n",
        "print(\"GPT labeled data\")\n",
        "display(df_gpt)\n",
        "\n",
        "# Our Nyan labeled data\n",
        "df_pos = pd.read_csv('./data/nyan.csv')[['text', 'other_posts', 'category']]\n",
        "print(\"Mined data from sources\")\n",
        "display(df_pos)"
      ],
      "metadata": {
        "id": "Btwz3E_94Dsm"
      },
      "id": "Btwz3E_94Dsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10263cd6-3f62-444d-a8b0-576b8b5b4f10",
      "metadata": {
        "tags": [],
        "id": "10263cd6-3f62-444d-a8b0-576b8b5b4f10"
      },
      "outputs": [],
      "source": [
        "# Prep gpt data\n",
        "# Hard filtering\n",
        "# Left for real life example\n",
        "df_gpt = df_gpt.iloc[:100] # You can delete this part if you want to run fully\n",
        "df_gpt = df_gpt.dropna()\n",
        "df_gpt['post_1'] = df_gpt['post_1'].apply(lambda x: np.nan if \"подписат\" in x.lower() else x)\n",
        "df_gpt['post_2'] = df_gpt['post_2'].apply(lambda x: np.nan if \"подписат\" in x.lower() else x)\n",
        "df_gpt = df_gpt.dropna()\n",
        "df_gpt['target'] = 1\n",
        "\n",
        "df_neg = generate_neg_sample(df_gpt)\n",
        "df_neg['target'] = 0\n",
        "df = pd.concat([df_gpt[['post_1', 'post_2', 'target']],df_neg]).sample(frac=1)\n",
        "df.to_csv('./data/4traingpt_seminar.csv', index=False)\n",
        "\n",
        "# For seminar we will use already prepared result\n",
        "df = pd.read_csv('./data/4traingpt.csv')\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a55129-46e6-4f77-aba8-2242eda83116",
      "metadata": {
        "tags": [],
        "id": "75a55129-46e6-4f77-aba8-2242eda83116"
      },
      "outputs": [],
      "source": [
        "# Prep mined from web data\n",
        "# Hard filtering\n",
        "# Left for real life example\n",
        "df_pos = df_pos.iloc[:100] # You can delete this part if you want to run fully\n",
        "df_pos['other_posts'] = df_pos['other_posts'].apply(lambda x: eval(x))\n",
        "df_pos['text'] = df_pos['other_posts']\n",
        "df_pos = df_pos.explode('other_posts')\n",
        "df_pos = df_pos.explode('text')\n",
        "df_pos['other_posts'] = df_pos['other_posts'].apply(lambda x: ' '.join([word if '@' not in word else '' for word in x.split()]))\n",
        "df_pos['text'] = df_pos['text'].apply(lambda x: ' '.join([word if '@' not in word else '' for word in x.split()]))\n",
        "df_pos['text'] = df_pos['text'].apply(lambda x: np.nan if \"подписат\" in x.lower() else x)\n",
        "df_pos['other_posts'] = df_pos['other_posts'].apply(lambda x: np.nan if \"подписат\" in x.lower() else x)\n",
        "df_pos['text'] = df_pos[['text', 'other_posts']].apply(lambda x: np.nan if x['text']==x['other_posts'] else x['text'], axis=1)\n",
        "df_pos = df_pos.dropna()\n",
        "df_pos = df_pos.rename({'text':'post_1', 'other_posts':'post_2'}, axis=1)\n",
        "df_pos['target'] = 1\n",
        "\n",
        "df_neg = generate_neg_sample(df_pos)\n",
        "df_neg['target'] = 0\n",
        "df = pd.concat([df_pos,df_neg]).sample(frac=1)[['post_1', 'post_2', 'target']]\n",
        "df.to_csv('./data/4trainnyan.csv', index=False)\n",
        "\n",
        "# For seminar we will use already prepared result\n",
        "df = pd.read_csv('./data/4trainnyan.csv')\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e719c4-2aaf-4328-8e5e-0ba147b01f78",
      "metadata": {
        "id": "53e719c4-2aaf-4328-8e5e-0ba147b01f78"
      },
      "source": [
        "## Training time!\n",
        "Real train loop took more than 3 hours. For simplicity of the seminar we will use only 10k pairs from our data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration (`CFG`)\n",
        "\n",
        "The `CFG` dictionary holds various hyperparameters and settings for our training process:\n",
        "\n",
        "* **`NUM_WORKERS`**: Number of sub-processes to use for data loading. You can use: 4.\n",
        "* **`BATCH_SIZE`**: The number of examples processed in one forward/backward pass during training. You can use: 64.\n",
        "* **`EPOCHS`**: The total number of times the training algorithm will iterate over the entire training dataset. You can use: 5.\n",
        "* **`SEED`**: A fixed number used to initialize random number generators (for sampling, shuffling, etc.) to ensure reproducibility. You can use: 42.\n",
        "* **`LR`**: The initial learning rate for the optimizer, controlling the step size during model updates. You can use: 1e-5.\n",
        "* **`SCHEDULER`**: The learning rate scheduler strategy (\"CosineAnnealingWarmRestarts\"). This dynamically adjusts the learning rate during training, potentially improving convergence.\n",
        "* **`T_0`**: Parameter for the CosineAnnealingWarmRestarts scheduler. You can use: 3.\n",
        "* **`min_lr`**: Minimum learning rate the scheduler can reduce to. You can use: 1e-6."
      ],
      "metadata": {
        "id": "JiwZuFo9GBso"
      },
      "id": "JiwZuFo9GBso"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9a3932-673f-480f-9db0-35b5a50ac755",
      "metadata": {
        "tags": [],
        "id": "da9a3932-673f-480f-9db0-35b5a50ac755"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    \"NUM_WORKERS\": <YOUR_CODE>,\n",
        "    \"BATCH_SIZE\": <YOUR_CODE>,\n",
        "    \"EPOCHS\": <YOUR_CODE>,\n",
        "    \"SEED\": <YOUR_CODE>,\n",
        "    \"LR\": <YOUR_CODE>,\n",
        "    \"SCHEDULER\": <YOUR_CODE>,\n",
        "    \"T_0\": <YOUR_CODE>,\n",
        "    \"min_lr\": <YOUR_CODE>\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "716f7035-8bd1-46f2-8304-f30f0a62e2f9",
      "metadata": {
        "tags": [],
        "id": "716f7035-8bd1-46f2-8304-f30f0a62e2f9"
      },
      "outputs": [],
      "source": [
        "class InputExample:\n",
        "    \"\"\"\n",
        "    Structure for one input example with texts, the label and a unique id\n",
        "    \"\"\"\n",
        "    def __init__(self, guid: str = '', texts: List[str] = None,  label: Union[int, float] = 0):\n",
        "        \"\"\"\n",
        "        Creates one InputExample with the given texts, guid and label\n",
        "\n",
        "\n",
        "        :param guid\n",
        "            id for the example\n",
        "        :param texts\n",
        "            the texts for the example.\n",
        "        :param label\n",
        "            the label for the example\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.texts = texts\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model: The `run` Function\n",
        "\n",
        "The `run` function is the core component that orchestrates the entire process of training. It takes a model and a data file as input and manages the data handling, setup, and execution of the training loop.\n",
        "\n",
        "Here's a high-level overview of what the `run` function does:\n",
        "\n",
        "1.  **Load and Prepare Data:**\n",
        "    * It begins by loading the specified CSV data file containing pairs of news texts and their binary labels (0 for non-duplicate, 1 for duplicate).\n",
        "    * It performs basic cleaning by removing any rows with missing values.\n",
        "    * **Crucially for this seminar:** It then applies a **sampling step**, selecting a limited number (up to 10,000 positive and 10,000 negative) of examples from the loaded data. This is done to create a smaller, more manageable dataset for faster training during the session, allowing us to see results without waiting for hours on large files.\n",
        "\n",
        "2.  **Split Data for Training and Validation:**\n",
        "\n",
        "3.  **Format Data for the Model:**\n",
        "    * The training data is converted into a list of `InputExample` objects, a format specifically required by the `sentence-transformers` library. Each `InputExample` bundles a text pair and its corresponding label.\n",
        "    * A PyTorch `DataLoader` is created from these `InputExample`s. The DataLoader manages batching and shuffling the training data during the training epochs.\n",
        "\n",
        "4.  **Set Up Training Components:**\n",
        "    * The appropriate loss function, **Contrastive Loss**, is initialized. This function calculates how well the model's embeddings distinguish between positive and negative pairs, guiding the learning process.\n",
        "    * A **Binary Classification Evaluator** is set up using the validation data. This tool will periodically calculate relevant metrics (like F1 score, accuracy) on the unseen validation data to monitor the model's progress and prevent overfitting.\n",
        "\n",
        "5.  **Execute Training:**"
      ],
      "metadata": {
        "id": "IBIchiXdHn5u"
      },
      "id": "IBIchiXdHn5u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini-Theory: Contrastive Loss\n",
        "\n",
        "**Contrastive Loss** is a widely used loss function in **Metric Learning**. Its primary goal is to train a model to learn an embedding function $f(x)$ that maps data points $x$ into a vector space such that the distance between embeddings reflects their similarity in the original data space.\n",
        "\n",
        "The core idea is simple:\n",
        "\n",
        "* **Similar** data points should have **small** distances between their embeddings.\n",
        "* **Dissimilar** data points should have **large** distances between their embeddings.\n",
        "\n",
        "Contrastive Loss achieves this by operating on **pairs** of data points:\n",
        "\n",
        "1.  **Positive Pairs:** Pairs of data points that are considered similar or related (e.g., two views of the same object, two paraphrased sentences about the same event). The loss function encourages the distance between their embeddings to be minimal.\n",
        "2.  **Negative Pairs:** Pairs of data points that are considered dissimilar or unrelated. For these pairs, the loss function encourages the distance between their embeddings to be large, specifically **greater than a certain margin ($m$)**.\n",
        "\n",
        "The loss function for a single pair $(x_i, x_j)$ with a binary label $y$ (where $y=1$ for a positive pair and $y=0$ for a negative pair) is typically defined as:\n",
        "\n",
        "$$L(x_i, x_j, y) = y \\cdot D(f(x_i), f(x_j)) + (1-y) \\cdot \\max(0, m - D(f(x_i), f(x_j)))$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $f(x)$ is the embedding function (our neural network).\n",
        "* $D(\\cdot, \\cdot)$ is a distance function (commonly squared Euclidean distance, $||f(x_i) - f(x_j)||_2^2$).\n",
        "* $m$ is the **margin**, a crucial hyperparameter.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "* If the pair is **Positive** ($y=1$), the first term $D(f(x_i), f(x_j))$ contributes to the loss, pushing the embeddings closer.\n",
        "* If the pair is **Negative** ($y=0$), the second term $\\max(0, m - D(f(x_i), f(x_j)))$ contributes. This term is zero if the distance $D$ is already greater than or equal to the margin $m$. It only penalizes the model if the negative pair's embeddings are **too close** (closer than $m$), pushing them further apart just enough to exceed the margin.\n",
        "\n",
        "By minimizing the sum of the losses over many positive and negative pairs, the model learns to create a useful embedding space for distinguishing similar from dissimilar items."
      ],
      "metadata": {
        "id": "_pIuGc6nJEnP"
      },
      "id": "_pIuGc6nJEnP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task:\n",
        "1. Fill in the number of samples for seminar (we recommend 10_000, because you dont want to wait too much).\n",
        "2. Create DataLoader for train_examples (dont forget to shuffle).\n",
        "3. Fill in evaluator.\n",
        "4. Fill in .fit() method.\n",
        "\n",
        "**Useful links**:\n",
        "1. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "2. https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html\n",
        "3. https://sbert.net/docs/sentence_transformer/training_overview.html"
      ],
      "metadata": {
        "id": "_o5wdspgKOKS"
      },
      "id": "_o5wdspgKOKS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab6900a-36d8-4e39-ba72-5f02c4ef31b5",
      "metadata": {
        "tags": [],
        "id": "4ab6900a-36d8-4e39-ba72-5f02c4ef31b5"
      },
      "outputs": [],
      "source": [
        "def run(model, file):\n",
        "    df = pd.read_csv(file).dropna()\n",
        "\n",
        "    # --- SEMINAR: Sample 1000 positive and 1000 negative examples (you dont want to wait 3 hours) ---\n",
        "    # Separate positive and negative samples\n",
        "    df_positive = df[df['target'] == 1].reset_index(drop=True)\n",
        "    df_negative = df[df['target'] == 0].reset_index(drop=True)\n",
        "\n",
        "    num_pos_samples = min(<YOUR_CODE>, len(df_positive))\n",
        "    num_neg_samples = min(<YOUR_CODE>, len(df_negative))\n",
        "\n",
        "    print(f\"Sampling {num_pos_samples} positive and {num_neg_samples} negative examples.\")\n",
        "\n",
        "    # Sample the desired number of examples from each category\n",
        "    sampled_pos = df_positive.sample(n=num_pos_samples, random_state=42).reset_index(drop=True)\n",
        "    sampled_neg = df_negative.sample(n=num_neg_samples, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Concatenate the sampled data and shuffle it\n",
        "    df = pd.concat([sampled_pos, sampled_neg]).sample(frac=1, random_state=42).dropna().reset_index(drop=True)\n",
        "\n",
        "    # --- END SEMINAR SAMPLING ---\n",
        "    train, val = train_test_split(df, train_size=0.8, stratify=df['target'], shuffle=True)\n",
        "    train = train.reset_index(drop=True)\n",
        "    val = val.reset_index(drop=True)\n",
        "\n",
        "    print(train.shape, val.shape)\n",
        "\n",
        "    train_examples = []\n",
        "    for i in range(train.shape[0]):\n",
        "        example = train.iloc[i]\n",
        "        train_examples.append(InputExample(texts=[example['post_1'], example['post_2']], label=example['target']))\n",
        "    train_loader = <YOUR_CODE>\n",
        "\n",
        "    evaluator = evaluation.BinaryClassificationEvaluator(sentences1=<YOUR_CODE>,\n",
        "                                                        sentences2=<YOUR_CODE>,\n",
        "                                                        labels=<YOUR_CODE>,\n",
        "                                                        batch_size=CFG['BATCH_SIZE'],\n",
        "                                                        show_progress_bar=True,\n",
        "                                                        name='eval_res',\n",
        "                                                        write_csv=True)\n",
        "    def cb(score, epoch, steps):\n",
        "        print(score, epoch, steps)\n",
        "\n",
        "    os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "    criterion = losses.ContrastiveLoss(model=model)\n",
        "    model.fit(\n",
        "        train_objectives=<YOUR_CODE>,\n",
        "        evaluator=<YOUR_CODE>,\n",
        "        epochs=CFG[\"EPOCHS\"],\n",
        "        show_progress_bar=True,\n",
        "        output_path='./model',\n",
        "        save_best_model=True,\n",
        "        callback=cb,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model_name = 'distiluse-base-multilingual-cased'\n",
        "\n",
        "model = SentenceTransformer(model_name, device='cuda:0')\n",
        "\n",
        "CFG[\"EPOCHS\"] = 2\n",
        "run(model, './data/4traingpt.csv')\n",
        "\n",
        "model = SentenceTransformer('./model', device='cuda:0')\n",
        "\n",
        "CFG[\"EPOCHS\"] = 5\n",
        "run(model, './data/4trainnyan.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4777763b-2de8-4b3c-baf2-81eb7901e3a4",
      "metadata": {
        "id": "4777763b-2de8-4b3c-baf2-81eb7901e3a4"
      },
      "source": [
        "# Calculate Metrics\n",
        "\n",
        "We want to evaluate the trained Sentence Transformer model's performance for news deduplication and find the optimal threshold for making predictions based on embedding similarity.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "* **Embedding Generation:** It first uses the trained `model` to generate vector embeddings for all unique texts in the dataset.\n",
        "* **Similarity Scoring:** For every pair of articles, it calculates the **cosine distance** between the embeddings of their respective texts. These scores represent how far apart the pairs are in the embedding space.\n",
        "* **Threshold Optimization:** It iterates through a range of possible threshold values (from 0.01 to 0.99). For each threshold, it simulates making predictions (based on whether the cosine distance is above or below the threshold) and calculates the F1 score by comparing these predictions to the true labels. It keeps track of the threshold that results in the highest F1 score. **In real life you can take thr from val logs of model**.\n",
        "* **Final Evaluation:** Using the best threshold found, it applies this cutoff to the cosine distance scores across the entire dataset to make the final binary predictions (duplicate/non-duplicate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ca1ac5-eeab-4355-9307-c65c9c1e95fb",
      "metadata": {
        "tags": [],
        "id": "c9ca1ac5-eeab-4355-9307-c65c9c1e95fb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/4trainnyan.csv')\n",
        "df_td = pd.DataFrame(df['post_1'].unique())\n",
        "df_td['emb'] = model.encode(df_td[0].tolist()).tolist()\n",
        "df = pd.merge(pd.merge(df, df_td.rename({0:'post_1', 'emb':'emb_1'}, axis=1), on='post_1', how='left'), df_td.rename({0:'post_2', 'emb':'emb_2'}, axis=1), on='post_2', how='left')\n",
        "df['emb_1'] = df['emb_1'].apply(lambda x: normalize([x]).ravel())\n",
        "df['emb_2'] = df['emb_2'].apply(lambda x: normalize([x]).ravel())\n",
        "df['cos_sim'] = df.apply(lambda x: pairwise_distances([x['emb_1']], [x['emb_2']], metric='cosine')[0][0], axis=1)\n",
        "df = df.dropna()\n",
        "df_td = df.copy()\n",
        "max_f1 = 0\n",
        "max_th = 0\n",
        "\n",
        "for th_int in tqdm(range(1, 100)):\n",
        "    th = th_int/100\n",
        "    df_td['ans'] = df_td['cos_sim'].apply(lambda x: int(x>=th))\n",
        "    f1 = f1_score(df_td['target'], df_td['ans'])\n",
        "    if f1 > max_f1:\n",
        "        max_f1 = f1\n",
        "        max_th = th\n",
        "\n",
        "df_td['ans'] = df_td['cos_sim'].apply(lambda x: int(x>=max_th))\n",
        "print(f\"F1: {f1_score(df_td['target'], df_td['ans'])}\\nAccuracy: {accuracy_score(df_td['target'], df_td['ans'])}\\nPrecision: {precision_score(df_td['target'], df_td['ans'])}\\nRecall: {recall_score(df_td['target'], df_td['ans'])}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "3a875eb2-b14a-4c08-bafe-b0a683b1f424"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}