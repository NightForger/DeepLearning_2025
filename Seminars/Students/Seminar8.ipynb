{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xprn6zNHkyyN",
        "mynpIjCBIEra"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 1 FGSM & PGD Attacks\n",
        "\n",
        "Both Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) are adversarial attack methods used to test the robustness of machine learning models, particularly those used in image classification. They work by subtly perturbing input data (like images) in a way that causes the model to misclassify it with high confidence – even though the change is often imperceptible to humans."
      ],
      "metadata": {
        "id": "TkucRb_HwdzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "xprn6zNHkyyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import copy\n",
        "import time\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "weights_path = hf_hub_download(repo_id=\"AAAAAA2121/weight_host\", filename=\"resnet18-cifar100.pt\")\n",
        "print(f\"Weights downloaded to: {weights_path}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Define transformations\n",
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar100_mean, cifar100_std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar100_mean, cifar100_std),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "def make_resnet18_for_cifar100(pretrained=True):\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "\n",
        "    # Critical architecture modifications for CIFAR\n",
        "    # 1. Change first conv layer from 7x7 to 3x3 with padding=1\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    # 2. Nullify first maxpool layer\n",
        "    model.maxpool = nn.Identity()\n",
        "\n",
        "    # Modify final layer\n",
        "    model.fc = nn.Linear(512, 100)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VutCpmW5JYPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c218f76a-e73b-4ebd-b314-466494a888fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:01<00:00, 87.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Train and Plot"
      ],
      "metadata": {
        "id": "aPJ1-rdZK4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task:\n",
        "* Epochs - 15\n",
        "* Loss - CrossEntropy\n",
        "* Resnet_lr = 0.1\n",
        "* Resnet_momentum = 0.9\n",
        "* Resnet_wd = 5e-4"
      ],
      "metadata": {
        "id": "UkLg6ii1qvIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "NUM_EPOCHS = <YOUR_CODE>\n",
        "CRITERION = <YOUR_CODE>\n",
        "\n",
        "# ResNet-specific parameters\n",
        "RESNET_LR = <YOUR_CODE>\n",
        "RESNET_MOMENTUM = <YOUR_CODE>\n",
        "RESNET_WD = <YOUR_CODE>\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, train_loader, test_loader, device, model_name):\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f'Training {model_name}', leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = CRITERION(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(test_loader, desc=f'Testing {model_name}', leave=False):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = CRITERION(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_epoch_loss = test_loss / test_total\n",
        "        test_epoch_acc = test_correct / test_total\n",
        "        history['test_loss'].append(test_epoch_loss)\n",
        "        history['test_acc'].append(test_epoch_acc)\n",
        "\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "        print(f'Test Loss: {test_epoch_loss:.4f} Acc: {test_epoch_acc:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if test_epoch_acc > best_acc:\n",
        "            best_acc = test_epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            print(f\"New best accuracy: {best_acc:.4f}\")\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "AEt-0yZXk33e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This was for you to remind how we write train loops and which parameters choose\n",
        "# But for seminar we will use pretrained edition\n",
        "resnet18 = torch.load(weights_path, weights_only=False) #Use model like one that we trained earlier (map_location=torch.device('cpu') if you run out of T4)"
      ],
      "metadata": {
        "id": "COrYUIPEJ4Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Attack Defining\n",
        "1. Fast Gradient Sign Method (FGSM)\n",
        "\n",
        "Concept: FGSM is a single-step adversarial attack. It calculates the gradient of the loss function with respect to the input image and then adds a small perturbation in the direction of the sign of that gradient. Essentially, it finds the direction in which a tiny change to the input will most quickly increase the loss (making the model more wrong).\n",
        "\n",
        "How it works:\n",
        "\n",
        "Calculate Gradient: Given an input image x, a target label y (the true label), and a machine learning model f, calculate the gradient of the loss function J(f(x), y) with respect to the input image x: ∇<sub>x</sub> J(f(x), y). This gradient tells you how much each pixel value affects the error of the prediction.\n",
        "Get the Sign: Take the sign of the gradient: sign(∇<sub>x</sub> J(f(x), y)). This results in a matrix of +1s and -1s indicating the direction to change each pixel to maximize the loss.\n",
        "Apply Perturbation: Add a small multiple (epsilon - denoted as ε) of the sign of the gradient to the original image: x' = x + ε * sign(∇<sub>x</sub> J(f(x), y)) . x' is the adversarial example.\n",
        "Clip (Optional but Recommended): Often, the perturbed image x' will have pixel values outside the valid range (e.g., 0-255 for images, or 0-1 normalised). Clipping ensures pixel values stay within this range: x' = clip(x', 0, 1)\n",
        "Epsilon (ε): This is a crucial parameter controlling the magnitude of the perturbation. A smaller epsilon means a less noticeable change to the image, but might not be enough to fool the model. A larger epsilon makes the attack more effective but also more detectable.\n",
        "\n",
        "Strengths: Simple and fast to compute.\n",
        "Weaknesses: Relatively weak compared to other attacks like PGD. Often easily defended against.\n",
        "\n",
        "\n",
        "2. Projected Gradient Descent (PGD)\n",
        "\n",
        "Concept: PGD is an iterative version of FGSM. Instead of taking just one step in the direction of the gradient, it takes multiple small steps, projecting the adversarial example back into a valid region after each step. It can be thought of as repeatedly applying FGSM with a constrained step size.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Initialization: Start with the original input image x.\n",
        "Iterate: For a specified number of iterations (T):\n",
        "\n",
        "1. Calculate Gradient: Calculate the gradient of the loss function with respect to the input image: ∇<sub>x</sub> J(f(x), y).\n",
        "\n",
        "2. Update Adversarial Example: Update the adversarial example by adding a small step in the direction of the gradient: x' = x + α * sign(∇<sub>x</sub> J(f(x), y)) (where α is the step size - analogous to epsilon in FGSM but per iteration).\n",
        "\n",
        "3. Projection: Project the updated adversarial example x' back onto a valid region around the original image x. This ensures the perturbation remains within a defined bound (often using an L-infinity norm constraint, meaning the maximum change for any single pixel is limited). The projection formula looks something like: x' = clamp(x', x - ε, x + ε) – this keeps each pixel within ±ε of its original value.\n",
        "\n",
        "4. Update x: Set x = x' for the next iteration.\n",
        "Output: After T iterations, the final x' is the adversarial example.\n",
        "\n",
        "Parameters:\n",
        "Step Size (α): Determines how much to move in the gradient direction in each iteration.\n",
        "Number of Iterations (T): How many times to repeat the process. More iterations generally lead to a stronger attack but increase computation time.\n",
        "Epsilon (ε): Defines the maximum allowable perturbation (same as in FGSM, but acts as the overall constraint).\n",
        "Strengths: Significantly more powerful than FGSM. Considered a strong first-order adversary and often used as a benchmark for evaluating robustness.\n",
        "Weaknesses: Computationally more expensive than FGSM due to the iterative nature.\n"
      ],
      "metadata": {
        "id": "NggYRF59H7fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:**\n",
        "1. Loss - Cross Entropy\n",
        "2. Write updating Adversarial example with function in markdown."
      ],
      "metadata": {
        "id": "e8FAsmQ9rY9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, images, labels, epsilon=0.05):\n",
        "    \"\"\" Fast Gradient Sign Method (FGSM) \"\"\"\n",
        "    images.requires_grad = True\n",
        "    outputs = model(images)\n",
        "    loss = <YOUR_CODE>(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Generate adversarial examples\n",
        "    perturbed_images = images + epsilon * images.grad.sign()\n",
        "    perturbed_images = torch.clamp(perturbed_images, 0, 1)  # Clip to [0,1] range\n",
        "    return perturbed_images.detach()\n",
        "\n",
        "def pgd_attack(model, images, labels, epsilon=0.05, alpha=0.01, num_iter=10):\n",
        "    \"\"\" Projected Gradient Descent (PGD) \"\"\"\n",
        "    perturbed_images = images.clone().detach()\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        perturbed_images.requires_grad = True\n",
        "        outputs = model(perturbed_images)\n",
        "        loss = <YOUR_CODE>(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update adversarial examples\n",
        "        adv_images = <YOUR_CODE>\n",
        "        eta = torch.clamp(adv_images - images, min=-epsilon, max=epsilon)  # Projection\n",
        "        perturbed_images = torch.clamp(images + eta, 0, 1).detach()  # Clip to [0,1]\n",
        "\n",
        "    return perturbed_images"
      ],
      "metadata": {
        "id": "burdRrM_J9Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Robustness Evaluation"
      ],
      "metadata": {
        "id": "6wLvpCgZKg86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_robustness(model, test_loader, device, attack=None, **attack_kwargs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if attack:\n",
        "            adv_images = attack(model, images, labels, **attack_kwargs)\n",
        "            outputs = model(adv_images)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate clean accuracy\n",
        "print(\"Evaluating clean accuracy...\")\n",
        "resnet_clean_acc = evaluate_robustness(resnet18, test_loader, device)\n",
        "\n",
        "# Evaluate FGSM attack\n",
        "print(\"\\nEvaluating FGSM attack...\")\n",
        "resnet_fgsm_acc = evaluate_robustness(resnet18, test_loader, device, fgsm_attack, epsilon=0.03)\n",
        "\n",
        "# Evaluate PGD attack\n",
        "print(\"\\nEvaluating PGD attack...\")\n",
        "resnet_pgd_acc = evaluate_robustness(resnet18, test_loader, device, pgd_attack, epsilon=0.03, alpha=0.01, num_iter=10)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\"ResNet-18\"],\n",
        "    \"Clean Accuracy\": [resnet_clean_acc],\n",
        "    \"FGSM Accuracy (ε=0.03)\": [resnet_fgsm_acc],\n",
        "    \"PGD Accuracy (ε=0.03)\": [resnet_pgd_acc],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(results_df[\"Model\"], results_df[\"Clean Accuracy\"], label=\"Clean\", alpha=0.7)\n",
        "plt.bar(results_df[\"Model\"], results_df[\"FGSM Accuracy (ε=0.03)\"], label=\"FGSM\", alpha=0.7)\n",
        "plt.bar(results_df[\"Model\"], results_df[\"PGD Accuracy (ε=0.03)\"], label=\"PGD\", alpha=0.7)\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Adversarial Robustness (CIFAR-100)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZGRZRDiRKhdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3: Defending Against Adversarial Attacks with Autoencoders\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Deep learning models, despite their power, are vulnerable to **adversarial attacks**. These are subtly modified inputs, often indistinguishable to humans, that cause the model to make incorrect predictions with high confidence. This vulnerability is a major concern for safety-critical applications like self-driving cars.\n",
        "\n",
        "One defense strategy involves using **Autoencoders (AE)**. An AE is trained to reconstruct clean data from potentially noisy or perturbed inputs. By passing an adversarial image through a trained AE, we aim to filter out the malicious perturbations, allowing a subsequent classifier to make the correct prediction.\n",
        "\n",
        "This notebook demonstrates:\n",
        "1.  Generating adversarial examples using the Fast Gradient Sign Method (FGSM).\n",
        "2.  Defining and using an Autoencoder to defend against these attacks.\n",
        "\n",
        "This practical session is based on the concepts and code from the repository: [https://github.com/anirudh9784/Adversarial-Attacks-and-Defences](https://github.com/anirudh9784/Adversarial-Attacks-and-Defences) and the associated research paper."
      ],
      "metadata": {
        "id": "ODSMJGGxH4ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "mynpIjCBIEra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow cleverhans huggingface_hub Pillow opencv-python-headless matplotlib numpy -q\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "import keras.models as models\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import orthogonal\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split # Used in original, but we'll slice for demo\n",
        "import cv2 # OpenCV for image loading/saving if needed, though PIL/Keras is often used"
      ],
      "metadata": {
        "id": "FBMhMlFeB4kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EliSchwartz/imagenet-sample-images.git dataset\n",
        "\n",
        "try:\n",
        "    weights_path = hf_hub_download(repo_id=\"AAAAAA2121/weight_host\", filename=\"AutoEncoderFinal.h5\")\n",
        "    print(f\"Weights downloaded to: {weights_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading weights: {e}\")\n",
        "    print(\"Please ensure the repository and file exist and are accessible.\")\n",
        "    weights_path = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2igd1znIfHt",
        "outputId": "bf51b13d-fb94-4cd4-da7c-421cb4180b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dataset' already exists and is not an empty directory.\n",
            "Weights downloaded to: /root/.cache/huggingface/hub/models--AAAAAA2121--weight_host/snapshots/79811c717de8ba35792d3e9041bfd33484c26641/AutoEncoderFinal.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vis_compare_images(val_x, adv_val_x, reconstructed_val_x=None):\n",
        "  examples = [3,4,5,8,10]\n",
        "  n_examples = len(examples) # Number of examples to show\n",
        "  plt.figure(figsize=(15, 6)) # Adjusted figure size\n",
        "  for j in range(n_examples):\n",
        "      i = examples[j]\n",
        "      # Original Image\n",
        "      ax = plt.subplot(3, n_examples, j + 1)\n",
        "      plt.imshow(val_x[i])\n",
        "      original_probs = pretrained_model.predict(val_x[i][None, ...])\n",
        "      _, label, conf = get_imagenet_label(original_probs)\n",
        "      plt.title(f\"Original\\n{label}\\n({conf:.2f})\")\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      # Adversarial Image\n",
        "      ax = plt.subplot(3, n_examples, j + 1 + n_examples)\n",
        "      plt.imshow(adv_val_x[i])\n",
        "      adversarial_probs = pretrained_model.predict(adv_val_x[i][None, ...])\n",
        "      _, adv_label, adv_conf = get_imagenet_label(adversarial_probs)\n",
        "      plt.title(f\"Adversarial\\n{adv_label}\\n({adv_conf:.2f})\")\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      if reconstructed_val_x is not None:\n",
        "        # Reconstructed Image\n",
        "        ax = plt.subplot(3, n_examples, j + 1 + 2 * n_examples)\n",
        "        plt.imshow(reconstructed_val_x[i])\n",
        "        reconstructed_probs = pretrained_model.predict(reconstructed_val_x[i][None, ...])\n",
        "        _, rec_label, rec_conf = get_imagenet_label(reconstructed_probs)\n",
        "        plt.title(f\"Reconstructed\\n{rec_label}\\n({rec_conf:.2f})\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout() # Adjust layout\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "uCjETnvQP5D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep\n",
        "**Task**:\n",
        "1. Define target size as 224x224."
      ],
      "metadata": {
        "id": "oJWrRZaxJAMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = []\n",
        "image_dir = 'dataset'\n",
        "target_size = <YOUR_CODE>\n",
        "error_count = 0\n",
        "\n",
        "image_files = glob.glob(os.path.join(image_dir, '*.*'))\n",
        "print(f\"Found {len(image_files)} files in {image_dir}.\")\n",
        "\n",
        "for img_path in tqdm(image_files):\n",
        "    try:\n",
        "        # Load image using tf.keras preferred method\n",
        "        img = tf.keras.utils.load_img(img_path, target_size=target_size)\n",
        "        img_array = tf.keras.utils.img_to_array(img)\n",
        "        # Normalize to [0, 1]\n",
        "        img_array = img_array / 255.0\n",
        "        all_images.append(img_array)\n",
        "    except Exception as e:\n",
        "        # Catch potential errors like non-image files or corrupt images\n",
        "        # print(f\"Skipping file {img_path}: {e}\")\n",
        "        error_count += 1\n",
        "\n",
        "if error_count > 0:\n",
        "      print(f\"\\nSkipped {error_count} potentially non-image files.\")\n",
        "\n",
        "all_images = np.array(all_images)\n",
        "print(f\"\\nSuccessfully loaded {len(all_images)} images.\")\n",
        "print(\"Dataset shape:\", all_images.shape)\n",
        "\n",
        "if len(all_images) >= 20: # Ensure we have enough images\n",
        "      val_x = all_images[:20] # Use first 20 images as validation set\n",
        "      print(\"Using first 20 images as validation set for demonstration.\")\n",
        "      print(\"Validation set shape:\", val_x.shape)\n",
        "else:\n",
        "      print(\"Warning: Not enough images loaded for validation split, using all loaded images.\")\n",
        "      val_x = all_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvfnXpJCJCJy",
        "outputId": "25d26b4b-0f8c-4220-e886-70e00d484296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 239 files in dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 239/239 [00:01<00:00, 152.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully loaded 239 images.\n",
            "Dataset shape: (239, 224, 224, 3)\n",
            "Using first 20 images as validation set for demonstration.\n",
            "Validation set shape: (20, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial Attack: Fast Gradient Sign Method (FGSM)\n",
        "\n",
        "FGSM is a simple yet effective **white-box** attack, meaning it requires knowledge of the model's parameters (specifically, its gradients). It works by perturbing an input image in the direction of the gradient of the loss function with respect to the input. The goal is to maximize the loss, thereby increasing the chance of misclassification.\n",
        "\n",
        "The FGSM perturbation is calculated as:\n",
        "\n",
        "$ \\text{adv\\_x} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)) $\n",
        "\n",
        "Where:\n",
        "* `adv_x`: The adversarial image.\n",
        "* `x`: The original input image.\n",
        "* `y`: The original input label (can be the true label or the model's prediction on `x`).\n",
        "* $\\epsilon$: A small multiplier to control the magnitude (visibility) of the perturbation.\n",
        "* $\\theta$: The model parameters.\n",
        "* $J(\\theta, x, y)$: The loss function used to train the model.\n",
        "* $\\nabla_x J(\\theta, x, y)$: The gradient of the loss with respect to the input image `x`.\n",
        "* $\\text{sign}(\\cdot)$: The sign function, which returns -1, 0, or 1 based on the sign of the input."
      ],
      "metadata": {
        "id": "jSRfSPY_IreA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:**\n",
        "1. Init pretrained MobileNetV2 with imagenet wieghts: https://keras.io/api/applications/mobilenet/ and freeze the weights.\n",
        "2."
      ],
      "metadata": {
        "id": "PfqET4BWlv02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained MobileNetV2\n",
        "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True, weights=<YOUR_CODE>)\n",
        "pretrained_model.trainable = <YOUR_CODE> # Freeze weights\n",
        "\n",
        "# Function to decode predictions\n",
        "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "def get_imagenet_label(probs):\n",
        "  return decode_predictions(probs, top=1)[0][0]\n",
        "\n",
        "# Create a model ending at the logits layer for FGSM\n",
        "logits_model = tf.keras.Model(pretrained_model.input, pretrained_model.layers[-1].output)\n",
        "\n",
        "def generate_adversarial_example(input_image_tensor, epsilon=0.1):\n",
        "  \"\"\"Generates an adversarial example using FGSM.\"\"\"\n",
        "  # Ensure input is a tf.Tensor\n",
        "  if not isinstance(input_image_tensor, tf.Tensor):\n",
        "      input_image_tensor = tf.convert_to_tensor(input_image_tensor, dtype=tf.float32)\n",
        "\n",
        "  # Add batch dimension if missing\n",
        "  if len(input_image_tensor.shape) == 3:\n",
        "      input_image_tensor = tf.expand_dims(input_image_tensor, axis=0)\n",
        "\n",
        "  adv_example = fast_gradient_method(\n",
        "      model_fn=logits_model,\n",
        "      x=input_image_tensor,\n",
        "      eps=epsilon,\n",
        "      norm=np.inf, # L-infinity norm\n",
        "      targeted=False # Untargeted attack\n",
        "  )\n",
        "  # Clip values to be in valid image range [0, 1]\n",
        "  adv_example = tf.clip_by_value(adv_example, 0.0, 1.0)\n",
        "  return adv_example # Return the single adversarial image tensor (remove batch dim)\n",
        "\n",
        "# Generate Adversarial Examples for Validation Set\n",
        "adv_val_x = []\n",
        "epsilon = 0.001\n",
        "print(f\"Generating adversarial examples for {len(val_x)} validation images...\")\n",
        "for i in tqdm(range(val_x.shape[0])):\n",
        "  original_image = val_x[i]\n",
        "  adv_image_tensor = generate_adversarial_example(original_image, epsilon)\n",
        "  # Remove the batch dimension before appending\n",
        "  adv_val_x.append(adv_image_tensor[0].numpy())\n",
        "\n",
        "adv_val_x = np.array(adv_val_x)\n",
        "print(\"Finished generating adversarial examples.\")\n",
        "print(\"Adversarial validation set shape:\", adv_val_x.shape)"
      ],
      "metadata": {
        "id": "myTCJ8szJNa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis_compare_images(val_x, adv_val_x)"
      ],
      "metadata": {
        "id": "bLfG65K1JVIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder Defense\n",
        "\n",
        "An Autoencoder (AE) is a type of neural network used for unsupervised learning, primarily for dimensionality reduction or feature learning. It consists of two main parts:\n",
        "\n",
        "1.  **Encoder:** Compresses the input data into a lower-dimensional latent representation (the \"code\" or \"bottleneck\").\n",
        "2.  **Decoder:** Reconstructs the original input data from the latent representation.\n",
        "\n",
        "**How it works for defense:**\n",
        "The key idea is to train the autoencoder to reconstruct *clean* images well, while potentially *filtering out* the small, high-frequency perturbations added by adversarial attacks. We train the AE using clean images as both input and target, or sometimes using noisy/adversarial images as input and their clean counterparts as the target.\n",
        "\n",
        "When an adversarial image is fed into the trained AE, the encoder compresses it, potentially discarding some of the adversarial noise structure. The decoder then attempts to reconstruct the *original*, clean image structure it learned during training.\n",
        "\n",
        "**Process:**\n",
        "`Adversarial Image` -> `Trained Autoencoder` -> `Reconstructed (Denoised) Image` -> `Classifier`\n",
        "\n",
        "The final classification is performed on the reconstructed image."
      ],
      "metadata": {
        "id": "WzPUX2QuJ7h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:**\n",
        "1. Use dropout 0.2 everywhere.\n",
        "2. Adam learning_rate - 0.002"
      ],
      "metadata": {
        "id": "uKDkG6I4mGgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
        "    \"\"\"Defines a Convolutional Layer block with LeakyReLU, Dropout, BatchNormalization.\"\"\"\n",
        "    prefix = f'block_{block_id}_'\n",
        "    x = layers.Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,kernel_initializer=kernel_init, name=prefix+'conv')(x)\n",
        "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
        "    x = layers.Dropout(<YOUR_CODE>, name=prefix+'drop')((x))\n",
        "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
        "    return x\n",
        "\n",
        "def Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
        "    \"\"\"Defines a Transpose Convolutional Layer block.\"\"\"\n",
        "    prefix = f'block_{block_id}_'\n",
        "    x = layers.Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,kernel_initializer=kernel_init, name=prefix+'de-conv')(x)\n",
        "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
        "    x = layers.Dropout(<YOUR_CODE>, name=prefix+'drop')((x))\n",
        "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
        "    return x\n",
        "\n",
        "# Autoencoder Model Definition\n",
        "def AutoEncdoer(input_shape):\n",
        "    \"\"\"Defines the Autoencoder model architecture.\"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Encoder part\n",
        "    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)\n",
        "    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)\n",
        "    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)\n",
        "    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)\n",
        "    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)\n",
        "    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)\n",
        "    conv7 = Conv2DLayer(conv6, 1024, 3, strides=2, padding='same', block_id=60) # Bottleneck\n",
        "\n",
        "    # Decoder part with skip connections\n",
        "    deconv0 = Transpose_Conv2D(conv7, 1024, 3, strides=2, padding='same', block_id=7)\n",
        "    skip0 = layers.concatenate([deconv0, conv6], name='skip0')\n",
        "    conv77 = Conv2DLayer(skip0, 512, 3, strides=1, padding='same', block_id=80)\n",
        "    deconv1 = Transpose_Conv2D(conv77, 256, 3, strides=2, padding='same', block_id=90)\n",
        "    skip1 = layers.concatenate([deconv1, conv5], name='skip1')\n",
        "    conv7_2 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)\n",
        "    deconv2 = Transpose_Conv2D(conv7_2, 128, 3, strides=2, padding='same', block_id=9)\n",
        "    skip2 = layers.concatenate([deconv2, conv3], name='skip2')\n",
        "    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)\n",
        "    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)\n",
        "    skip3 = layers.concatenate([deconv3, conv2], name='skip3')\n",
        "    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)\n",
        "    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)\n",
        "    skip4 = layers.concatenate([deconv4, conv1], name='skip4')\n",
        "    # Final layer to reconstruct the image\n",
        "    conv10 = layers.Conv2D(3, 3, strides=1, padding='same', activation='sigmoid', kernel_initializer=orthogonal(), name='final_conv')(skip4)\n",
        "    return models.Model(inputs=inputs, outputs=conv10)\n",
        "\n"
      ],
      "metadata": {
        "id": "WWSqwTIfIx24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape_ae = (224, 224, 3)\n",
        "autoencoder_model = AutoEncdoer(input_shape_ae)\n",
        "\n",
        "model_opt = Adam(learning_rate=<YOUR_CODE>)\n",
        "autoencoder_model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "autoencoder_model.summary() # Print model summary"
      ],
      "metadata": {
        "id": "ZqLwpcg3B4mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Autoencoder Weights\n",
        "autoencoder_model.load_weights(weights_path)\n",
        "\n",
        "# Defend: Reconstruct Adversarial Images\n",
        "reconstructed_val_x = autoencoder_model.predict(adv_val_x)\n",
        "print(\"Reconstruction complete.\")\n",
        "print(\"Reconstructed set shape:\", reconstructed_val_x.shape)"
      ],
      "metadata": {
        "id": "4GMQQAqCB4pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Visualize Results\n",
        "Compare the Original images, the Adversarial images generated by FGSM, and the images Reconstructed by the Autoencoder. Observe how the autoencoder attempts to clean the adversarial perturbations, and sometimes too smoothing the image."
      ],
      "metadata": {
        "id": "RDzhRw46Kx7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vis_compare_images(val_x, adv_val_x, reconstructed_val_x)"
      ],
      "metadata": {
        "id": "6uf1lKUFPL9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}